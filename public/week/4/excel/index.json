[{"content":"Of Macroscopes and Microscopes Goals for this week This week, I want you to begin to have a \u0026lsquo;dialogue with your texts\u0026rsquo;, wherein you learn to use three tools of varying levels of complexity to get distant and close reads on a body of information. Iterating through this cycle, will help you know your data better and see the patterns that would otherwise be lost in all the noise.\nWhen you do this kind of work, it is super important that you make a note of what you expect to find before you do the analysis; as Matthew Lincoln reminds us, we have to do this so that we don\u0026rsquo;t post-facto rationalize what we find as something we already knew\u0026hellip;\nListen Read   Tim Hitchcock, 2013 \u0026lsquo;Big Data for Dead People\u0026rsquo; Historyonics\n  Matthew Lincoln, 2015 \u0026lsquo;Confabulation in the humanities\u0026rsquo; blog\n  Do   Begin with some Excel basics\n  Load some data into Voyant-Tools and explore it\n  Consider the power of collocation with AntConc\n  See if a topic model might usefully surface some patterns.\n  And if you\u0026rsquo;re really ambitious, try your hand at some data publishing with Datasette and Heroku.\n  When/if you run into trouble, take screenshots (google how to do that for your particular machine) and these can be uploaded into your repository as well. Indeed, you should also keep track of any files you create as part of your weekly work in your repo: these are evidence!\n  With tech work, if it doesn\u0026rsquo;t come together in about 30 minutes, it won\u0026rsquo;t come in an hour. So take a break. Close the laptop. Call somebody up for help. Find another pair of eyes to look at the problem. I don\u0026rsquo;t want to hear that you labored heroically for 2 hours to do something. Jump into our social space and ask for advice. Remember: it\u0026rsquo;s not how many you do, it\u0026rsquo;s that you pushed yourself that matters.\r Record and Reflect   As you did in previous weeks, make another notes.md entry and put it in your repo for week 4.\n  In your journal.md for this week, think about\n  Submit work You can submit the link to your work on this form\n","description":"Of Macroscopes and Microscopes","id":0,"section":"week","tags":null,"title":"Instructions: May 25","uri":"https://craftingdh.netlify.com/week/4/instructions/"},{"content":"A Brief Introduction to Digital History Goals for this week  getting our Github accounts set up and making our first log entries getting Hypothes.is set up getting Zotero installed   learn how to describe what\u0026rsquo;s gone wrong, and to figure out how to fix it   explore our ignorance: what is dighist?  Listen Feed for the podcast here.\n  Read   Sharon Leon, 2016.\u0026lsquo;Returning Women to the History of Digital History\u0026rsquo; [bracket]\n  \u0026lsquo;History Can Be Open Source: Democratic Dreams and the Rise of Digital History\u0026rsquo; AHR Open Review (You can see comments on each paragraph by the peer reviewers by clicking on the \u0026lsquo;comments\u0026rsquo; pane on the right hand side of the interface)\n  .twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  Since pretty much everyone under quarantine has to use digitized primary sources, I thought it might be useful to point to #digitalarchives that *explicitly* acknowledge \u0026amp; caution users abt #archivalsilences in their contents, and describe their work to rectify them.\nA thread.\n\u0026mdash; Amalia Skarlatou Levi (@amaliasl) April 2, 2020  Read the full thread by Amalia Skarlatou Levi and explore links of interest; annotate anything interesting you find with Hypothes.is in our group keeping in mind what you\u0026rsquo;ve already heard/read. Share anything interesting you\u0026rsquo;ve found in our social space. A handy getting-started approach to annotation is to think of the three W\u0026rsquo;s: the weird, the wonderful, the worrying. Also add anything you read or anything interesting you find to your Zotero library.  Do   Follow the instructions for Setting Up Github, Setting up Hypothesis, Setting up Zotero, and asking for help. You should read the discord page if you\u0026rsquo;re new to that platform.\n  Log your reflection in your appropriate github repository\n  When/if you run into trouble, take screenshots (google how to do that for your particular machine) and these can be uploaded into your repository as well. Indeed, you should also keep track of any files you create as part of your weekly work in your repo: these are evidence!\n  With tech work, if it doesn\u0026rsquo;t come together in about 30 minutes, it won\u0026rsquo;t come in an hour. So take a break. Close the laptop. Call somebody up for help. Find another pair of eyes to look at the problem. I don\u0026rsquo;t want to hear that you labored heroically for 2 hours to do something. Jump into our social space and ask for advice.\r Record and Reflect   Being a digital historian means keeping track of what you\u0026rsquo;ve done, as a gift to your future self (ie, so that when you come back to something, you can pick up where you left off). Make a new text document, and put into it any new terms you\u0026rsquo;ve encountered, commands you used, error messages you encountered, websites that helped, and so on: this document is a scrap book, as it were. Bullet points and memos-to-self are fine. Put this text document into your week one repo on github, along with any other files or digital things you happen to make. Call it your \u0026lsquo;notes.md\u0026rsquo;.\n  In a new text document, jot down some reflections -narrative, or bullet points, both are fine. Call it \u0026lsquo;journal.md\u0026rsquo; This document also goes into your week one repo on github. Detail any issues you had with getting started, any parts that caused you difficulty. If you got any error messages while trying to get set up, copy those into your reflection; google them. Do you find any websites that help you? What kind of \u0026lsquo;failures\u0026rsquo; might you have encountered this week?\n  Drawing on your annotations of what you\u0026rsquo;ve read (and/or your notes from what you\u0026rsquo;ve listened to), discuss your idea of what \u0026lsquo;digital history\u0026rsquo; might be prior to starting this course, and think through whether any of the materials we\u0026rsquo;ve seen this week confirm or upset those notions. What kinds of \u0026lsquo;digital silences\u0026rsquo; did you encounter? What has excited you? Finally, where do you want to go, with digital history?\nSubmit Work You can submit the link to your work on this form\n","description":"An overview of DigHist","id":1,"section":"week","tags":null,"title":"Instructions: May 4","uri":"https://craftingdh.netlify.com/week/1/instructions/"},{"content":"Basic Tools Goals for this week  learn some of the ways historical information is made available digitally, paying attention to disparities of labour, and of tech  Listen Read Something given. That\u0026rsquo;s a nice way of thinking about it. Of course, much of the data that we are \u0026lsquo;given\u0026rsquo; wasn\u0026rsquo;t really given willingly. When we topic model Martha Ballard\u0026rsquo;s diary, did she give this to us? Of course, she couldn\u0026rsquo;t have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Johanna Drucker famously framed it as data versus capta, or \u0026lsquo;things captured.\u0026rsquo;\nThis week, we\u0026rsquo;re thinking about some of the ways historical materials find their ways online, and we\u0026rsquo;re learning some of the ways we can pull that information onto our own machines for our own research. Read these two pieces discussing the way the Transcribe Bentham project organized the digitization of Bentham\u0026rsquo;s papers.\n  Causer, Tim, Grint, Kris, Sichani, Anna-Maria and Terras, Melissa, ”Making such bargain: Transcribe Bentham and the quality and cost-effectiveness of crowdsourced transcription’, Digital Scholarship in the Humanities, 33:3 2018, pp. 467-87 . Open access version.\n  Causer, Tonra, \u0026amp; Wallace, 2012. Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham LLC 27.2\n  Do Try to work through as many of these exercises as you can. Don\u0026rsquo;t worry too much if you get stuck: the point is to push yourself beyond your current level of ability. You are not grade on how many of these you do, but rather on how you push your development as a digital historian: and that journey looks different for each of you.\n Download and install the Sublime Text Editor Install Conda Download web materials with wget Retrieve materials from the Chronicling America API Use OCR (Object Character Recognition) to turn images of text into text  Bonus (for when you have time, energy, and inclination):\n Download videos with youtube-dl Transcribe audio from videos and other sources automatically   When/if you run into trouble, take screenshots (google how to do that for your particular machine) and these can be uploaded into your repository as well. Indeed, you should also keep track of any files you create as part of your weekly work in your repo: these are evidence!  With tech work, if it doesn\u0026rsquo;t come together in about 30 minutes, it won\u0026rsquo;t come in an hour. So take a break. Close the laptop. Call somebody up for help. Find another pair of eyes to look at the problem. I don\u0026rsquo;t want to hear that you labored heroically for 2 hours to do something. Jump into our social space and ask for advice. Remember: it\u0026rsquo;s not how many you do, it\u0026rsquo;s that you pushed yourself that matters.\r Record and Reflect   As you did for week one, make another notes.md entry and put it in your repo for week 2.\n  In your journal.md for this week, think about, who pays for work to end up online? Who does the work? What are some of the ethical dimensions of doing this work? Does Carleton give you any resources for getting those materials onto your own machine in formats you can read? What are some of the barriers to accessing the resources that Carleton does make available to you? Where do you fit into this digital history machine? Put your journal.md into your github repo for week 2.\n  Submit work You can submit the link to your work on this form\n","description":"Basic Tools","id":2,"section":"week","tags":null,"title":"Instructions: May 11","uri":"https://craftingdh.netlify.com/week/2/instructions/"},{"content":"The 18th is the Long Weekend. Take a break from your school work, if you can.\r Basic Tools Encore Goals for this week 80% of digital work is transforming and cleaning materials so that they can become amenable to digital investigation. Your goals this week are to learn\n the utility of regular expressions for cleaning and organizing an OCR\u0026rsquo;d document ways of further tidying text with Open Refine exploratory visualization with network analysis  Listen Read   James Baker, 2017. \u0026lsquo;The Soft Digital History That Underpins My Book\u0026rsquo; https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/\n  James Baker, 2017. \u0026lsquo;https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/'\n  Do  Transform a text file using Regular Expressions Clean it further with Open Refine Visualize the results through networks   When/if you run into trouble, take screenshots (google how to do that for your particular machine) and these can be uploaded into your repository as well. Indeed, you should also keep track of any files you create as part of your weekly work in your repo: these are evidence!  With tech work, if it doesn\u0026rsquo;t come together in about 30 minutes, it won\u0026rsquo;t come in an hour. So take a break. Close the laptop. Call somebody up for help. Find another pair of eyes to look at the problem. I don\u0026rsquo;t want to hear that you labored heroically for 2 hours to do something. Jump into our social space and ask for advice. Remember: it\u0026rsquo;s not how many you do, it\u0026rsquo;s that you pushed yourself that matters.\r Record and Reflect   As you did for week one, make another notes.md entry and put it in your repo for week 3.\n  In your journal.md for this week, think about all of the little decisions you have to make about the material as you clean and transform it. What is the value of/for historians (regular historians) showing their work, as opposed to, or in contrast to, what digital historians have to do? Where is the scholarly value in showing your work - and contrast this with how you\u0026rsquo;ve written history to date.\n  Submit work You can submit the link to your work on this form\n","description":"Basic Tools Encore","id":3,"section":"week","tags":null,"title":"Instructions: May 19","uri":"https://craftingdh.netlify.com/week/3/instructions/"},{"content":"Get some quantitative stuff, a census or something. Show some basics (nb not teaching stats)\nbasic counting in R. Basic plots\nSome Basic Counting and Plotting in R Start up your RStudio, and make a new R script. The first thing we\u0026rsquo;re going to do is get set up so that we can import some data directly from the web. We use the RCurl package to do that:\n1 2  install.packages(\u0026#34;RCurl\u0026#34;) library(\u0026#34;RCurl\u0026#34;)   (Remember to hit the run button for each line.)\nWe\u0026rsquo;re going to reach out and grab a table of data (the colonial newspapers database you encountered earlier) and import it into R. We do that like this:\n1  x \u0026lt;- getURL(\u0026#34;https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv\u0026#34;, .opts = list(ssl.verifypeer = FALSE))   Notice you now have a variable in your \u0026lsquo;Global Environment\u0026rsquo; pane. We need to extract the data from that, which we do with this:\n1  documents \u0026lt;- read.csv(text = x, col.names=c(\u0026#34;Article_ID\u0026#34;, \u0026#34;Newspaper Title\u0026#34;, \u0026#34;Newspaper City\u0026#34;, \u0026#34;Newspaper Province\u0026#34;, \u0026#34;Newspaper Country\u0026#34;, \u0026#34;Year\u0026#34;, \u0026#34;Month\u0026#34;, \u0026#34;Day\u0026#34;, \u0026#34;Article Type\u0026#34;, \u0026#34;Text\u0026#34;, \u0026#34;Keywords\u0026#34;), colClasses=rep(\u0026#34;character\u0026#34;, 3), sep=\u0026#34;,\u0026#34;, quote=\u0026#34;\u0026#34;)   We read the csv file from x, and create the columns into which the data is poured; all of this is now in documents. When we only want information from a particular column, we modify the variable slightly (eg. head(documents$Keywords) would return the first few rows of the information in the keywords column).\nNow we can do some things. Let\u0026rsquo;s count the number of documents by the city in which they were published:\n1 2  counts \u0026lt;- table(documents$Newspaper.City) counts   The first counts creates the variable; the second counts on its own reveals what\u0026rsquo;s inside the variable.\nNow let\u0026rsquo;s plot that:\n1  barplot(counts, main=\u0026#34;Cities\u0026#34;, xlab=\u0026#34;Number of Articles\u0026#34;)   The plot will appear in the bottom right pane of RStudio. You can click on \u0026lsquo;zoom\u0026rsquo; to see the plot in a popup window. You can also export it as a PNG or PDF file. Clearly, we’re getting an Edinburgh/Glasgow perspective on things. And somewhere in our data, there’s a mispelled ‘Edinbugh’. Do you see any other error(s) in the plot? How would you correct it(them)?\nLet\u0026rsquo;s do the same thing for year, and count the number of articles per year in this corpus:\n1 2  years \u0026lt;- table(documents$Year) barplot(years, main=\u0026#34;Publication Year\u0026#34;, xlab=\u0026#34;Year\u0026#34;, ylab=\u0026#34;Number of Articles\u0026#34;)   There’s a lot of material in 1789, another peak around 1819, againg in the late 1830s. We can ask ourselves now: is this an artefact of the data, or of our collection methods? This would be a question a reviewer would want answers to. Let’s assume for now that these two plots are ‘true’ — that, for whatever reasons, only Edinburgh and Glasgow were concerned with these colonial reports, and that they were particulary interested during those three periods. This is already an interesting question that we as historians would want to explore.\nTry making some more visualizations like this of other aspects of the data. What other patterns do you see that are worth investigating?\nThis page shows you the code for some other basic visualizations. See if you can make some more visualizations. I\u0026rsquo;ve created a tarsus.txt file and a unicorn.txt file so that you can see how his code works (although you might wish to open both files in your sublime text editor and add more rows of data; careful: columns are separated by tabs.)\n","description":"Of Macroscopes and Microscopes","id":4,"section":"week","tags":null,"title":"Excel \u0026 R","uri":"https://craftingdh.netlify.com/week/4/excel/"},{"content":"Telling the Compelling Story Goals for this week Listen Read I asked on twitter, (as you do),\n.twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  most compelling works of digital history? go!\n\u0026mdash; Shawn Graham (@electricarchaeo) April 13, 2020 Explore some of the suggestions. Or, take a look at this piece by former Carleton U public history MA student, Cristina Woods:\n Songs of the Ottawa \u0026lsquo;The Ottawa River\u0026rsquo;s History Re-told in Musical Notes\u0026rsquo; https://ottawacitizen.com/news/local-news/the-ottawa-rivers-history-re-told-in-musical-notes/  Or explore The Digital Panopticon.\nDo  When/if you run into trouble, take screenshots (google how to do that for your particular machine) and these can be uploaded into your repository as well. Indeed, you should also keep track of any files you create as part of your weekly work in your repo: these are evidence!  With tech work, if it doesn\u0026rsquo;t come together in about 30 minutes, it won\u0026rsquo;t come in an hour. So take a break. Close the laptop. Call somebody up for help. Find another pair of eyes to look at the problem. I don\u0026rsquo;t want to hear that you labored heroically for 2 hours to do something. Jump into our social space and ask for advice. Remember: it\u0026rsquo;s not how many you do, it\u0026rsquo;s that you pushed yourself that matters.\r Record and Reflect   As you did for the previous weeks, make another notes.md entry and put it in your repo for week 5.\n  In your journal.md for this week, think about\n  Submit work You can submit the link to your work on this form\n","description":"Telling the Compelling Story","id":5,"section":"week","tags":null,"title":"Instructions: June 1","uri":"https://craftingdh.netlify.com/week/5/instructions/"},{"content":"Introduction Cost = $0.\nAnaconda is a platform for data science work. By installing Anaconda, you get access to a series of tools for working in both the Python and R programming languages, plus excellent visual interfaces that reduce (some of) the pain of programming.\nWhen we do work in python, we sometimes need to add more lego-pieces to what we\u0026rsquo;re doing, to accomplish specific tasks. Anaconda comes with very nearly - but not all - of the pieces you might need. The thing is, different tasks might require the same pieces, and if you\u0026rsquo;ve ever fought with a sibling over who gets the good lego piece, well, that\u0026rsquo;s what can happen inside your machine. Anaconda keeps everything playing nice by allowing us to create environments with copies of the pieces we need, separate from other copies, so that there are no conflicts.\nAnaconda also gives us some excellent user interfaces for doing our work - juypter notebooks and R Studio - for instance. In this course, we\u0026rsquo;ll use R Studio a bit, and we\u0026rsquo;ll take a look at some jupyter notebooks, but we\u0026rsquo;ll mostly use python at the command line.\nDownload Download Anaconda From Here\nDouble-click the downloaded file to install.\nAccept all of the defaults during the installation. (More info on the installation process is here).\nAnaconda comes with something called \u0026lsquo;Anaconda Navigator\u0026rsquo;, which looks like this:\nPowershell and Terminal We may occasionally launch some of these apps (eg, Jupyter Notebooks, R Studio). For now, we\u0026rsquo;ll be using the command line. Windows users in particular: we\u0026rsquo;ll be using the \u0026lsquo;powershell\u0026rsquo;. You can click on the \u0026lsquo;Powershell Prompt\u0026rsquo; in Anaconda Navigator. You can also find it by searching your pc for \u0026lsquo;Anaconda Powershell\u0026rsquo;.\n  When I ask you to use the command prompt, Windows users: I mean anaconda powershell.\n  Mac users can use the terminal (which you can find under applications -\u0026gt; other -\u0026gt; terminal).\n  Going forward, when I want you to enter code at the command prompt or terminal prompt, I will signal this by starting the line with a $, like this:\n$ echo \u0026quot;this is what it looks like\u0026quot;\nYou do not type the $.\nOpen a command prompt or terminal now and confirm that anaconda is installed:\n$ conda --version\nand that python is installed:\n$ python --version\nif you get an error message, then anaconda did not install correctly or Windows users you might not be in anaconda powershell.\nWhen we type \u0026lsquo;conda\u0026rsquo; or \u0026lsquo;python\u0026rsquo; or indeed \u0026lsquo;echo\u0026rsquo; or anything else at the prompt, we are telling the computer that this is a command. If we want the machine to run a particular python program we\u0026rsquo;ve written, we would invoke python followed by the program name, eg example.py so that the machine knows to open example.py with python.\nNavigating the Commandline When you click on a file in your windows explorer or in mac\u0026rsquo;s finder, you click through nested folders, right? The path you take through those folders will look something like this: username\\example-folder\\a-subfolder\\file.txt, although finder or explorer by default don\u0026rsquo;t show you that.\nWhen you open anaconda powershell or the terminal, how do you know where you are at? You print the working directory:\n$ pwd\nand that will give you the path to where you are working. To see what\u0026rsquo;s inside the folder, you can:\n$ ls list files, on a Mac or\n$ dir directory, on a PC\nFiles will have an extension (eg, .txt, .doc) while directories won\u0026rsquo;t. To go into a directory, we change directories:\n$ cd subfolder\nand we can go back up a level:\n$ cd ..\nMy terminal, demonstrating a bit of moving around. Also some of the contents of my machine.\n","description":"instructions","id":6,"section":"week","tags":null,"title":"Setting up Anaconda","uri":"https://craftingdh.netlify.com/week/2/installing-anaconda/"},{"content":"If any of the instructions below are unclear, annotate this page with hypothesis while being logged into our course reading group. If you spot someone asking for help and you can offer advice, respond to the annotation. Introduction Github is a code sharing website often used by digital historians. \u0026lsquo;Git\u0026rsquo; is a program that takes snapshots of the current state of a folder, and stores them in sequence, allowing you to revert your changes to an earlier state. It also allows you to create branches, or duplicates of your folder, so you can experiment. If you like the results of your work, you can merge that branch back into your original.\nGithub therefore is a hub for sharing these git snapshots.\nA branch (a copy, a duplicate) of your work uploaded to Github could therefore be copied to say my account (a fork); then I might download to my machine to work on it (I\u0026rsquo;ve pulled it). Once I\u0026rsquo;m happy with my changes, I commit them (save them to the sequence of changes that Git tracks), and then I could push those changes to the fork in my account. I would then send you a pull notice, asking you to pull my changes back to your account; you could then decide whether or not to merge.\nFor our purposes, you will use Github mostly as a place to put your reflections or other pieces of work. I might sometimes fork your work, pull it down onto my machine, make changes that I commit, push it back online, and ask you to pull the changes back. But that won\u0026rsquo;t happen very often. It takes a while to get comfortable with this.\nGet your account  Got to github.com and sign up for an account. You don\u0026rsquo;t have to use your real name. (Protip: You might want to set up an email just for signing up for things.)  Verify your account.  Select the free tier (nb: I will never require you to pay for any reading, or any software. If something wants you to pay, stop and ask for help).  Skip telling Github anything about yourself.  Do the verification email thing. Once you hit the verification link in your email, you\u0026rsquo;ll be brought back to Github to make a new repository:  Give it a reasonable name; tick the \u0026lsquo;initialize with a readme\u0026rsquo; box, and hit the green commit button:  And on the final page, hit the \u0026lsquo;dismiss\u0026rsquo; button in the \u0026lsquo;Github Actions box\u0026rsquo;  Ta Da! You now have a github account, and you\u0026rsquo;ve created your first repository. Going Forward create a new repository for each week\u0026rsquo;s work/reflection. You can create a new repository from the plus sign in the top right corner:\nnb Remember to tick off the \u0026lsquo;initialize with a readme.md\u0026rsquo; file.\nOnce you\u0026rsquo;ve done that, you\u0026rsquo;ll be at your Github user page:\nSetting up a \u0026lsquo;repository\u0026rsquo; for your work A \u0026lsquo;repository\u0026rsquo; is just a folder that you\u0026rsquo;ve shared on Github. There are two ways to do this; the easy way and the more complex way. Luck you, you have already done the easy way - you selected the initialize with a readme, and it\u0026rsquo;s already present in your browser!\nTo create new repositories, just click on the + button on the top right of your Github page when you\u0026rsquo;re logged in. You might want to go ahead now and create repositories for week 2 through to week 6. Remember to tick off the initialize with a readme box. But if you didn\u0026rsquo;t tick the initialize box, you\u0026rsquo;ve embarked on the more complex way. In the screenshot below, I created a new repository but I forgot to initialize it, and now I\u0026rsquo;m looking at this page:\nIf this is you, do not despair.\nWhen you\u0026rsquo;ve forgotten to initialize a new repo:  If you have a PC, click on this link to download and install git. Make a new directory on your computer with the same name as the repo you created above. In my example, that would be week-two. On a PC, right-click on the folder and select \u0026lsquo;open a command prompt here\u0026rsquo;. On a Mac, go to System Preferences, select Keyboard \u0026gt; Shortcuts \u0026gt; Services. Look for \u0026lsquo;New Terminal at Folder\u0026rsquo; and tick the box. Open your finder; find the folder you created, right-click and select \u0026lsquo;open Terminal here\u0026rsquo;. Do you see where, in your browser at github, it says ...or create a new repository on the command line? Type in each line exactly as it is there (beginning with echo), hitting enter at the end of each line, in order. If the command works, you\u0026rsquo;ll just be presented with the next prompt. The computer only ever responds when there is output to print - which often means only when there is an error message to report.  Ta da! You can now go to github.com\\\u0026lt;your-user-name\u0026gt;\\your-repo and you\u0026rsquo;ll see it all there tickety boo.\nMaking a new text file on Github You can make a new text file by clicking on the \u0026lsquo;create new file\u0026rsquo; button; remember to always use .md as the file extension. You can specify headers, links, images, bullets, blockquotes and so on by using markdown conventions\nThe two videos below might be a bit clearer on youtube itself.\n  Uploading a file into Github You can add new files from your computer by dragging and dropping them into the main repository. At the end of this video, I show you how to display the image in the text of the reflection.\n  Going further Github can also be used to run an entire website, generated from your text files. In fact, that\u0026rsquo;s how I built this course website! The following three links are from my HIST4806a 2020 seminar on digital history and museums; they will walk you through how to use your Github account to serve up a professional scholarly website; you can use this scholarly website to host your reflections and course work, if you want. Follow these three pages in order:\n  Building Your Own Scholarly Website\n  Customizing Your Scholarly Website\n  Updating Your Scholarly Website\n  ","description":"instructions","id":7,"section":"week","tags":null,"title":"Setting up your Github","uri":"https://craftingdh.netlify.com/week/1/github/"},{"content":"Introduction A regular expression (also called regex) is a powerful tool for finding and manipulating text. At its simplest, a regular expression is just a way of looking through texts to locate patterns. When you search a website for instance (use ctrl+f in most applications to search, by the way), the search box finds exact matches; there\u0026rsquo;s no room for fuzziness. A regular expression on the other hand can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it\u0026rsquo;s spelled. As long as you can describe the pattern you\u0026rsquo;re looking for, regular expressions can help you find it. Once you\u0026rsquo;ve found your patterns, they can then help you manipulate your text so that it fits just what you need.\nRegular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple ‘regexes’ will be easy. Regular expressions can often be used right inside the \u0026lsquo;Find and Replace\u0026rsquo; box in many text and document editors, such as Sublime Text, Atom, or Notepad++. You cannot use Microsoft Word, however!\nNB In text editors, you have to indicate that you wish to do a regex search. In Sublime Text, you open the search panel from the \u0026lsquo;Find\u0026rsquo; menu (or use the shortcut) and then you need to tick the box that has .* in the search panel to enable regular expression searches.\nSome basic principles Protip: there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search “regular expression Canadian postal code” and learn what ‘formula’ to search for to find them.\nAlso, here is a cheatsheet for regular expressions in Sublime Text.\nLet\u0026rsquo;s say you\u0026rsquo;re looking for all the instances of \u0026ldquo;cat\u0026rdquo; or \u0026ldquo;dog\u0026rdquo; in your document. When you type the vertical bar on your keyboard (it looks like |, shift+backslash on windows keyboards), that means \u0026lsquo;or\u0026rsquo; in regular expressions. So, if your query is dog|cat and you press \u0026lsquo;find\u0026rsquo;, it will show you the first time either dog or cat appears in your text.\nIf you want to replace every instance of either \u0026ldquo;cat\u0026rdquo; or \u0026ldquo;dog\u0026rdquo; in your document with the world \u0026ldquo;animal\u0026rdquo;, you would open your find-and-replace box, put dog|cat in the search query, put animal in the \u0026lsquo;replace\u0026rsquo; box, hit \u0026lsquo;replace all\u0026rsquo;, and watch your entire document fill up with references to animals instead of dogs and cats.\nThe astute reader will have noticed a problem with the instructions above; simply replacing every instance of \u0026ldquo;dog\u0026rdquo; or \u0026ldquo;cat\u0026rdquo; with \u0026ldquo;animal\u0026rdquo; is bound to create problems. Simple searches don\u0026rsquo;t differentiate between letters and spaces, so every time \u0026ldquo;cat\u0026rdquo; or \u0026ldquo;dog\u0026rdquo; appear within words, they\u0026rsquo;ll also be replaced with \u0026ldquo;animal\u0026rdquo;. \u0026ldquo;catch\u0026rdquo; will become \u0026ldquo;animalch\u0026rdquo;; \u0026ldquo;dogma\u0026rdquo; will become \u0026ldquo;animalma\u0026rdquo;; \u0026ldquo;certificate\u0026rdquo; will become \u0026ldquo;certifianimale\u0026rdquo;. In this case, the solution appears simple; put a space before and after your search query, so now it reads:\ndog | cat\nWith the spaces, \u0026ldquo;animal\u0026rdquo; replace \u0026ldquo;dog\u0026rdquo; or \u0026ldquo;cat\u0026rdquo; only in those instances where they\u0026rsquo;re definitely complete words; that is, when they\u0026rsquo;re separated by spaces.\nThe even more astute reader will notice that this still does not solve our problem of replacing every instance of \u0026ldquo;dog\u0026rdquo; or \u0026ldquo;cat\u0026rdquo;. What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters.\n\\\u0026lt;\nmeans the beginning of a word. In some programs, you might use this alternative:\n\\b\nso if you search for \\\u0026lt;cat , (or, as it may be, \\bcat )it will find \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;catch\u0026rdquo;, and \u0026ldquo;catsup\u0026rdquo;, but not \u0026ldquo;copycat\u0026rdquo;, because your query searched for words beginning with \u0026ldquo;cat\u0026rdquo;. For patterns at the end of the line, you would use:\n\\\u0026gt; or \\b\nagain. If you therefore searched:\ncat\\\u0026gt;\nit will find \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;copycat\u0026rdquo;, but not \u0026ldquo;catch,\u0026rdquo; because your query searched for words ending with -\u0026ldquo;cat\u0026rdquo;.\nRegular expressions can be mixed, so if you wanted to find words only matching \u0026ldquo;cat\u0026rdquo;, no matter where in the sentence, you\u0026rsquo;d search for\n\\\u0026lt;cat\\\u0026gt;\nwhich would find every instance. And, because all regular expressions can be mixed, if you searched for\n\\\u0026lt;cat|dog\\\u0026gt;\nand replaced all with \u0026ldquo;animal\u0026rdquo;, you would have a document that replaced all instances of \u0026ldquo;dog\u0026rdquo; or \u0026ldquo;cat\u0026rdquo; with \u0026ldquo;animal\u0026rdquo;, no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \u0026ldquo;gray\u0026rdquo; or \u0026ldquo;grey\u0026rdquo;, instead of the search query\ngray|grey\nyou could type\ngr(a|e)y\ninstead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \u0026ldquo;that dog\u0026rdquo; or \u0026ldquo;that cat\u0026rdquo;, you would search for:\n(that dog)|(that cat)\nNotice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for.\nThe period character . in regular expressions directs the search to just find any character at all. For example, if we searched for:\nd.g\nthe search would return \u0026ldquo;dig\u0026rdquo;, \u0026ldquo;dog\u0026rdquo;, \u0026ldquo;dug\u0026rdquo;, and so forth.\nAnother special character from our cheat sheet, the plus symbol + instructs the program to find any number of the previous character. If we search for\ndo+g\nit would return any words that looked like \u0026ldquo;dog\u0026rdquo;, \u0026ldquo;doog\u0026rdquo;, \u0026ldquo;dooog\u0026rdquo;, and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying\n(do)+g\nwould return \u0026ldquo;dog\u0026rdquo;, \u0026ldquo;dodog\u0026rdquo;, \u0026ldquo;dododog\u0026rdquo;, and so forth.\nCombining the plus + and period . characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for\nd.+g\nfor example, might return \u0026ldquo;dried fruits are g\u0026rdquo;, because the string begins with \u0026ldquo;d\u0026rdquo; and ends with \u0026ldquo;g\u0026rdquo;, and has various characters in the middle. Searching for simply .+ will yield query results that are entire lines of text, because you are searching for any character, and any amount of them.\nParentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what\u0026rsquo;s called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for\n(dogs)( and )(cats)\nwhich would find all instances of \u0026ldquo;dogs and cats\u0026rdquo; in your document, your program would remember \u0026ldquo;dogs\u0026rdquo; is group 1, \u0026ldquo;and\u0026rdquo; is group 2, and \u0026ldquo;cats\u0026rdquo; is group 3. Sublime Text remembers them as \\1, \\2, and \\3 for each group respectively.\nIf you wanted to switch the order of \u0026ldquo;dogs\u0026rdquo; and \u0026ldquo;cats\u0026rdquo; every time the phrase \u0026ldquo;dogs and cats\u0026rdquo; appeared in your document, you would type\n(dogs)( and )(cats)\nin the \u0026lsquo;find\u0026rsquo; box, and\n\\3\\2\\1\nin the \u0026lsquo;replace\u0026rsquo; box. That would replace the entire string with group 3 (\u0026ldquo;cats\u0026rdquo;) in the first spot, group 2 (\u0026rdquo; and \u0026ldquo;) in the second spot, and group 1 (\u0026ldquo;dogs\u0026rdquo;) in the last spot, thus changing the result to \u0026ldquo;cats and dogs\u0026rdquo;.\nThe vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online.\nLetters of the Republic of Texas The correspondence of the Republic of Texas, and independent state from 1835 to 1846 was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR\u0026rsquo;d by Google, and is now available as a text file from the Internet Archive. You can see the OCR\u0026rsquo;d text at archive.org. We are going to use it to practice our regex skills because it is an example of a historical network we might want to analyze later with network analysis. We are going to grab the index from that file, and transform it using regex.\nEntries in the index look like this:\nSam Houston to A. B. Roman, September 12, 1842 101 Sam Houston to A. B. Roman, October 29, 1842 101 Correspondence for 1843-1846 — Isaac Van Zandt to Anson Jones, January 11, 1843 103 We are going to use regex to tidy this up, delete some parts, and end up with data that looks like this:\nSam Houston, A. B. Roman, September 12 1842 Sam Houston, A. B. Roman, October 29 1842 Isaac Van Zandt, Anson Jones, January 11 1843 The change doesn\u0026rsquo;t look like much, and you might think to yourself, \u0026lsquo;hey, I could just do that by hand\u0026rsquo;. You could but it\u0026rsquo;d take you ages, and if you made a mistake somewhere, are you sure you could do this consistently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it.\n  We will use the curl command at the command prompt to grab the file; this is a related command to wget whom you\u0026rsquo;ve already met. Mac users should already have this installed; Windows users can open up Anaconda PowerShell and type conda install -c anaconda curl to get it.\n  At the command line, type the following curl command:\n  $ curl http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt \u0026gt; texas.txt\nThe curl command downloads the txt file and the the \u0026gt; pushes the result of the command to a file called texas.txt.\n Open texas.txt in Sublime Text, and open the Find menu and hit the Replace option; this opens the find and replace panel at the bottom of your window. Make sure to press the .* button in that panel to turn on regular expression search.\n  Delete everything in this file that isn\u0026rsquo;t the table of letters. The table starts with ‘Sam Houston to J. Pinckney Henderson, December 31, 1836 51’ and ends with ‘Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582’. Your file will now have approximately 2000 lines in it.\n  First thing we\u0026rsquo;re going to do is identify any lines that indicate correspondence. We\u0026rsquo;re going to look for the word \u0026lsquo;to\u0026rsquo;. In fact, we don\u0026rsquo;t just want to find \u0026ldquo;to\u0026rdquo;, but the entire line that contains it. We assume that every line that contains the word \u0026ldquo;to\u0026rdquo; in full is a line that has relevant letter information, and every line that does not is one we do not need.\n  You learned earlier that the pattern .+ returns any amount of text, no matter what it says, and that \\b indicates a word boundary. Thus, the pattern we want looks like this .+\\bto\\b.+. Don\u0026rsquo;t search yet.\nWe want to mark these lines off as special, so let\u0026rsquo;s add a tilde ~ before each of the lines that look like letters. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like the following\n(.+\u0026lt;to\u0026gt;)\nand the entire line is placed within a parenthetical group. Since this the first group in our search expression, we can replace that group with \\1 and put the tilde in front of it like so: ~\\1.\nDo this\nsearch for: (.+\\\u0026lt;to\\\u0026gt;)\nreplace with: ~\\1\nAfter running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. The search string to find all lines which don\u0026rsquo;t begin with tildes is \\n[^~].+  Within a set of square brackets [] the carrot ^ means search for anything that isn\u0026rsquo;t within these brackets (in this case, the tilde ~). The .+ as before means search for every remaining character in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters. We search for the pattern, and leave the replace blank; this will delete the lines that do not begin with a tilde.\nDo this\nsearch for: \\n[^~].+\nreplace with:\nTo turn this text file into a csv suitable for network analysis, we\u0026rsquo;ll want to separate it out into one column for Sender, one for Recipient, and one for Date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There\u0026rsquo;s also usually a comma separating the month-date and the year, which we\u0026rsquo;ll get rid of as well. In the end, the first line should go from looking like the following:  ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51\nto looking like the following:\nSam Houston, J. Pinckney Henderson, December 31 1836\nRead through before you do anything. You will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex:\n[0-9]{4}\nWe can find any digit between 0 and 9 by searching for [0-9], and {4} will find four of them together. Now extend that search out by appending .+ to the end of the query; as seen before, it will capture the entire rest of the line. The following query:\n`[0-9]{4}.+`  will return, for example, \u0026ldquo;1836 51\u0026rdquo;, \u0026ldquo;1839 52\u0026rdquo;, and \u0026ldquo;1839 53\u0026rdquo; from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in the following:\n `, [0-9]{4}.+`  which will return \u0026ldquo;, 1836 51\u0026rdquo;, \u0026ldquo;, 1839 52\u0026rdquo;, etc.\nThe next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after \u0026lsquo;year\u0026rsquo;, but not the year or the space before it. Thus our query will look like the following:\n`(,)( [0-9]{4})(.+)`  with the comma as the first group \\1, the space and the year as the second \\2, and the rest of the line as the third \\3. Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the replace look like? You only want to keep the second group.\n  It\u0026#39;ll look like this  search: (,)( [0-9]{4})(.+)\nreplace: \\2\nNow go ahead and do that.\n   Find the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.\n  Finally, to separate the Sender and Receiver by a comma, we find all instances of the word \u0026ldquo;to\u0026rdquo; and replace it with a comma. Although we used \\b and \\b to denote the beginning and end of a word earlier in the lesson, we don\u0026rsquo;t exactly do that here. We include the space preceding \u0026ldquo;to\u0026rdquo; in the regular expression, as well as the  \\b to denote the word ending. Once we find instances of the word and the space preceding it, to\\b we replace it with a comma ,.\n  Do this\nsearch: (\\b to \\b)\nreplace: ,\n You now have a document that contains senders, recipients, and the date in three columns separated by commas. Well done! You\u0026rsquo;ll need to insert a line right at line 1 though to make this clear: at the top of the file, add a new line that simply reads \u0026ldquo;Sender, Recipient, Date\u0026rdquo;.\n  You may notice that some lines still do not fit our criteria. Line 22, for example, reads \u0026ldquo;Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \u0026ldquo;. It has an incomplete date; we don\u0026rsquo;t need to worry about these for our purposes.\n  More worrisome are lines, like 61 \u0026ldquo;Copy and summary of instructions United States Department of State, \u0026quot; which include none of the information we want. We can get rid of these lines later in a spreadsheet.\nThe only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \u0026ldquo;A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\u0026rdquo;. Notice that our second column, the name of the Recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for Sender, two for Recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The following query:\n.+,.+,.+,\nwill show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth. You can just \u0026lsquo;find all\u0026rsquo; and then Sublime will show you the lines to fix manually.\nPhew That was a lot of work. Save your file with a new name: correspondence.csv. You now have a document that can be visualized as a network and analyzed as such.\nBut it\u0026rsquo;s still pretty messy - it was, after all, OCR\u0026rsquo;d in the first place. We\u0026rsquo;ll fix that in the open refine exercise.\n","description":"Basic Tools","id":8,"section":"week","tags":null,"title":"REGEX","uri":"https://craftingdh.netlify.com/week/3/regex/"},{"content":"Text Analysis in a Nutshell   \u0026hellip;so let\u0026rsquo;s take a look at Voyant Tools:\nAn Introduction to Voyant   \u0026ldquo;The gateway drug to the digital humanities.\u0026rdquo; That\u0026rsquo;s quite a statement - but it\u0026rsquo;s true! What makes Voyant Tools so compelling, in my view, is that it encourages us to play. To take a step back, and turn the knobs and dials and to see what happens next. Voyant encourages us to zoom in and out, looking at the very broad macroscopic and then jump into the micro, and back again. Each tool panel in Voyant can be embedded into a new document, so that you can share what you find, or so that you can write an argument and your readers can explore your data at the same time.\nSo let\u0026rsquo;s do that. Dr. Melodee Beels of Loughborough University in the UK has been studying the ways early newspapers reprinted articles from their competitors (Here is a presentation she gave on her research from a few years ago). In the course of this work, she has put together a database of these articles. One ancillary question that Dr. Beels is interested in are the ways British newspapers framed Britain\u0026rsquo;s relationship with its colonies.\nA subset of her data is available here where each row is a unique article, and each column is field describing some of the metadata for that article. Can you determine anything about what broad patterns are in them? Not really, eh? Not without reading and making notes on every single row. So let\u0026rsquo;s take a distant read instead.\n Now, Voyant can read one row at a time in a table, and consider it a unique document - if the table is in Microsoft Excel format. I converted that csv to xlsx and I\u0026rsquo;ve made it available for you here. Right-click on that link, and \u0026lsquo;copy link location\u0026rsquo;. At Voyant Tools paste that link in the \u0026lsquo;Add Texts\u0026rsquo; box but don\u0026rsquo;t hit \u0026lsquo;Reveal\u0026rsquo; yet.  Click on the options button at the top right. We have to tell Voyant how to parse our table. We are going to tell it that we are uploading a table by making changes under the \u0026lsquo;tables\u0026rsquo; option. The newspaper articles themselves are in the 10th column of our spreadsheet, while the author metadata is in the 2nd column (the name of the newspaper) and the \u0026lsquo;title\u0026rsquo; we\u0026rsquo;ll create by combining the date columns. Thus, enter 10 in content, 2 in author, and 6+7+8 for title.  Hit \u0026lsquo;ok\u0026rsquo; and then hit \u0026lsquo;reveal\u0026rsquo;. You should see something rather like this:  Make a note of your corpus id; you\u0026rsquo;ll find it in the URL, after the ?corpus=. Here\u0026rsquo;s the URL for the version I made: https://voyant-tools.org/?corpus=ea1868d7f1fbece8f0f5538c23a3128e. You can return to your corpus or share your corpus with someone else by giving them that URL. As you explore Voyant, note how the URL changes to reflect the name of the tool, and the settings you applied.  Explore the Newspaper Corpus Before you explore, make a note of some of the questions you might like to ask, knowing what you know about how this corpus was put together, and what you think you might know about how Britain regarded its colonies in the 18th and 19th centuries.\nNormally, you would frame your questions and your hypotheses about the data before you started to collect it, because these hypotheses help you understand what evidence you\u0026rsquo;d need to gather, and would help you understand when to stop gathering the data. Obviously, we are constrained here because we are using someone else\u0026rsquo;s corpus: but maybe you can still see some questions worth asking? Reusing primary research is not something that we are comfortable doing, in History; we much prefer to redo the original research each time because we are all Heroic Lone Scholars. But that\u0026rsquo;s not really a good use of everyone\u0026rsquo;s time and energy. There are times when reusing someone\u0026rsquo;s corpus or data or workflow can be used to generate new insights, or confirm existing ones; this is something historians have to get used to doing.\nRockwell and Sinclair recommend \u0026lsquo;entering into a dialogue with a text\u0026rsquo;. They write,\n One way to think about how you can study a text is to think about entering into a conversation with the text through Voyant. Think about questions you might ask about the text like:\n What is this about? What words would I expect to see as describing the text? What does this text say about something that matters to me like “friendship”? What words in the Cirrus word cloud make sense, and what words are anomalous? How does the language of the text change over the span of the text? Are some words more important in the beginning and some at the end? Could there be framing words?\nText analysis tools like Voyant grew out a tradition of developing concordances for important texts like the bible or Shakespeare’s plays. Concordances were printed tools like indexes that allowed a preacher or scholar look up a word like “friendship” and see all the instances across the bible in one place. She could then think through friendship and prepare a sermon or paper discussing the theme. With Voyant you can now study any electronic text in a similar fashion. Try it!   Try identifying themes in the materials you\u0026rsquo;re looking at. Beside the word \u0026lsquo;cirrus\u0026rsquo; (eg, Voyant\u0026rsquo;s wordcloud tool) there is the word \u0026lsquo;Terms\u0026rsquo;. If you click that, you\u0026rsquo;ll see the same information but this time represented as a table with counts of the number of times the word appears in the corpus, and a small \u0026lsquo;spark line\u0026rsquo; graph showing roughly where the term is most prominent (since our corpus is organize in chronological order, the left side of the spark line is the oldest material, and the right side is the newest).\nDoes anything jump out at you? Click on the term. What changes in Voyant?\nIn the \u0026lsquo;trends\u0026rsquo; box, click on one of the high points; the \u0026lsquo;Contexts\u0026rsquo; panel will change to show you \u0026lsquo;key words in context\u0026rsquo; for that term, or five words to the left, and five words to the right, which enable you to start doing a close reading of the particular document.\nThus Voyant Tools enables you to cycle from distant to close reading and back again! Play with some of the customizations or other tools, and see if you can find any interesting patterns. Export anything interesting you find by hitting the button I labelled \u0026lsquo;embed\u0026rsquo; in the image below (\u0026lsquo;export\u0026rsquo;), and then selecting \u0026lsquo;HTML Snippet\u0026rsquo;. This snippet can be saved in your notes.md file and uploaded to Github; Github will render your widget as html!\nA dialog box will appear, and give you some html like this to save into your file:\n\u0026lt;iframe style='width: 530px; height: 315px;' src='https://voyant-tools.org/tool/CorpusTerms/?corpus=ea1868d7f1fbece8f0f5538c23a3128e'\u0026gt;\u0026lt;/iframe\u0026gt;\nNote how the URL contains the name of the tool, and the path to your corpus. When this is displayed as html, it will look like this, a fully interactive widget!\n If you double-click on anything in that widget, a new window will open bringing you to the full version of my corpus.\nVoyant is an excellent tool because it enfolds the reader in the process of making meaning from the data.\nAdd words to the stop list\nYou can add words to the \u0026lsquo;stoplist\u0026rsquo; tp tell Voyant to filter those words out - typically, you\u0026rsquo;ll see in the wordcloud some words that shouldn\u0026rsquo;t be considered, given the context of your materials - \u0026lsquo;newspaper\u0026rsquo; maybe, in this example. To add words to the stoplist,\n Select the \u0026lsquo;options\u0026rsquo; button in the word cloud panel. Select \u0026lsquo;English\u0026rsquo;, and then \u0026lsquo;Edit List\u0026rsquo;. You can add words, one line per word, and then \u0026lsquo;confirm\u0026rsquo;.\n  Build Your Own Corpus  Search the Chronicling America database for articles of interest; say you\u0026rsquo;re interested in how Canada has been represented to the everyday reader of newspapers in the US. Use what you learned in week 2 to assemble the data. Arrange each article in a spreasheet (use Microsoft Excel or Google Sheets), such that you have the following:     date headline article-text     1964 Man Bites Dog Anytown, USA. A man bit a dog yesterday\u0026hellip;   more rows of data    Save it as .xlsx. Import it into Voyant. Share your corpus with your classmates. Do some initial explorations - does anything jump out at you?\nGoing Further Voyant Tools lives on a server in Montreal. When we use its web interface, we are transmitting data back and forth from our machine to the machine in Montreal. There\u0026rsquo;s no necessary guarantee that anything we share couldn\u0026rsquo;t be intercepted somehow. If you are working with sensitive materials - oral history transcripts, say - you ought not to be using a web-based tool to analyze them. There is also the issue of storage. Materials that are uploaded to Voyant Tools can and do get deleted eventually if they are not regularly accessed.\nThe solution is to download and run Voyant Tools yourself! Just as you did with Open Refine, you can have Voyant run in a virtual server on your own machine, and you can then interact with it through your browser as if you were using the main Voyant Tools. The instructions for downloading and installing Voyant Tools on your own machine are here. Give it a shot! With Voyant Tools on your own machine, you do not need to worry about sensative materials finding their way onto the web.\n","description":"Of Macroscopes and Microscopes","id":9,"section":"week","tags":null,"title":"Voyant","uri":"https://craftingdh.netlify.com/week/4/voyant/"},{"content":" audacity sonification  ","description":"Telling the Compelling Story","id":10,"section":"week","tags":null,"title":"Sound","uri":"https://craftingdh.netlify.com/week/5/sound/"},{"content":"   While watching this video, turn on the closed captions.\nCreate an account at hypothes.is:\nGet the Chrome app or the Firefox bookmarklet:\nMake sure you\u0026rsquo;re logged in:\nThen join our HIST3814o reading group. If you happen to be reading something, and see an existing annotation that interests you, hit the \u0026lsquo;reply\u0026rsquo; button on the annotation to start a conversation! Sometimes, the person to whom you\u0026rsquo;re replying might be a previous year\u0026rsquo;s student, but that\u0026rsquo;s ok; they might enter into conversation with you. Sometimes, it might be a person who isn\u0026rsquo;t a Carleton student but is following along with the course. That\u0026rsquo;s ok too. Be respectful!\nThere is a student guide to Hypothes.is behind this link. Read that for the full information how using Hypothesis.\n","description":"instructions","id":11,"section":"week","tags":null,"title":"Setting up your Hypothesis","uri":"https://craftingdh.netlify.com/week/1/hypothesis/"},{"content":"The following combines elements of Ian Milligan\u0026rsquo;s tutorial at The Programming Historian, as well as Kellen Kurschinski\u0026rsquo;s.\nIntroduction Wget is a program for downloading materials from the web. It is extremely powerful: if we do it wrong we can look like an attacker, or worse, download the entire internet! We use this program on the command line. We will cover some of the basics, and then we will create a little program that uses wget to download materials from Library and Archives Canada.\nIn what follows, push yourself until you get stuck. I\u0026rsquo;m not interested in how far you get, but rather in how you document what you are able to do, how you look for help, how you reach out to others - or how you help others over the bumps. I know also that you all have lots of other claims on your time. Reading through all of this and making notes on what you do/don\u0026rsquo;t understand is fine too.\r Installation Mac Users\nYou will need a tool called \u0026lsquo;homebrew\u0026rsquo; to obtain wget. Homebrew is a \u0026lsquo;package manager\u0026rsquo;, or a utility that retrieves software from a single, \u0026lsquo;official\u0026rsquo; (as it were) source. To install home brew, copy the command below and enter it at your terminal:\n$ /usr/bin/ruby -e \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026quot; Then, make sure everything is set up correctly with brew, enter: $ brew doctor\nAssuming all has gone well, get wget: $ brew install wget\nThen test that it installed: $ wget If it has installed, you\u0026rsquo;ll get the message -\u0026gt; missing URL.. If it hasn\u0026rsquo;t you\u0026rsquo;ll see -\u0026gt; command not found.\nWindows\nGo to this website and right-click, \u0026lsquo;save target as\u0026rsquo; the file for the 32-bit version 1.20.3 EXE file.\nMove this file out of your Downloads folder into your C:\\Windows directory. That way, when you type wget at the command prompt, Windows will find it.\nBasic Usage Wget expects you to enter the URL to a webpage or some other online file. How do websites organize things? Milligan writes:\n Let’s take an example dataset. Say you wanted to download all of the papers hosted on the website ActiveHistory.ca. They are all located at: http://activehistory.ca/papers/; in the sense that they are all contained within the /papers/ directory: for example, the 9th paper published on the website is http://activehistory.ca/papers/historypaper-9/. Think of this structure in the same way as directories on your own computer: if you have a folder labeled /History/, it likely contains several files within it. The same structure holds true for websites, and we are using this logic to tell our computer what files we want to download.\n So let\u0026rsquo;s try to do that. At the command prompt or terminal, let\u0026rsquo;s make a new directory to do our work in:\n$ mkdir wget-activehistory $ cd wget-activehistory and let\u0026rsquo;s grab the index page from the papers directory:\n$ wget http://activehistory.ca/papers/\nta da! Look around in that directory, see what got downloaded.\nBut that was only one file. We\u0026rsquo;re going to add some more flags to the command to tell wget to recursively follow links (using the -r flag) in that folder but to only follow the links that lead to destinations within the folder (using the -np meaning \u0026lsquo;no parent\u0026rsquo;). Otherwise we could end up grabbing materials five steps away from this site! (If we did want materials outside of where we started, we\u0026rsquo;d use the -l flag, \u0026lsquo;links\u0026rsquo;). Finally, we don\u0026rsquo;t want to be attacking the site demanding gimme gimme gimme materials. We use the -w flag to wait between requests, and to --limit-rate=20k to narrow the bandwidth our request requires.\n -r recursive -np no-parent -l links beyond domain we started in -w wait time between requests to the server --limit-rate= limit the bandwidth for our request (which necessarily slows down how long it\u0026rsquo;ll take to perform the request)  Altogether, our command now looks like this:\n$ wget -r -np -w 2 --limit-rate=20k http://activehistory.ca/papers/\nGive that a try!\nUsing wget with a list of urls Let us assume that you were very interested in the history of health care in this country. Through the Library and Archives Canada search interface, you\u0026rsquo;ve found the Laura A. Gamble fonds, a nurse originally from Wakefield Quebec (just north of Ottawa).\nYou click through, and find the first image of her diary; if you right-click on that image and select view image you find that the file path to the image: http://data2.archives.ca/e/e001/e000000422.jpg.\nNow, if Library and Archives Canada had an API for their collection (an \u0026lsquo;application programming interface\u0026rsquo;, or a set of commands that we could use on our end to deduce the locations of the information we want), we could just figure out the urls for each image of the diary in that fonds. But they don\u0026rsquo;t. Turns out, the urls we want run from \u0026hellip;422 to \u0026hellip;425 (but try entering other numbers at the end of that URL: you will retrieve who-knows-what!):\n In sublime text, create a new file and paste these urls into it; save the file as urls.txt  http://data2.archives.ca/e/e001/e000000422.jpg http://data2.archives.ca/e/e001/e000000423.jpg http://data2.archives.ca/e/e001/e000000424.jpg http://data2.archives.ca/e/e001/e000000425.jpg Now, we know that we can pass an indivual url to wget and wget will retrieve it; we can also pass urls.txt to wget, and wget will grab every file in turn!\nTry this at the terminal/command prompt:  $ wget -i urls.txt -r --no-parent -nd -w 2 --limit-rate=100k\n(google for wget options)\nUsing python to generate a list of urls Now, let\u0026rsquo;s try something a bit more complex. It\u0026rsquo;s one thing to manually copy and paste urls into a file, but what if we want a lot of files? You could just set wget to crawl directories, but that can make you look like an attacker, it can clutter your own machine with files you don\u0026rsquo;t want, and it can mess with your bandwidth and data caps. Sometimes though we can suss out the naming pattern for files, and so write a small program that will automatically write out all of the urls for us.\nConsider the 14th Canadian General Hospital war diaries. The URLs of this diary go from http://data2.archives.ca/e/e061/e001518029.jpg to http://data2.archives.ca/e/e061/e001518109.jpg. That\u0026rsquo;s 80 pages.\n make a new directory for our work - at the command prompt or terminal, $ mkdir war-diaries Create a new file in Sublime Text. I\u0026rsquo;ll give you the script to paste in there, and then I\u0026rsquo;ll explain what it\u0026rsquo;s doing.  1 2 3 4 5 6  urls = \u0026#39;\u0026#39;; f=open(\u0026#39;urls.txt\u0026#39;,\u0026#39;w\u0026#39;) for x in range(8029, 8110): urls = \u0026#39;http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n\u0026#39; % (x) f.write(urls) f.close   First, we\u0026rsquo;re creating an empty \u0026lsquo;bin\u0026rsquo; or varialbe called \u0026lsquo;urls\u0026rsquo;. Then, we create a variable called f for file, and tell it to open a new text file called \u0026lsquo;urls\u0026rsquo;. Then we set up a loop with the for command that will iterate over the 80 values from 8029 to 8110. The next line contains our pattern, the full url right up until e0151. The last little bit, the %d takes the number of the current iteration and pastes that in. The \\n means \u0026lsquo;new line\u0026rsquo; so that when we iterate through this again, we\u0026rsquo;ve moved the cursor down one line. Then with f.write(urls) we put the pattern into the file. Once we\u0026rsquo;ve finished - we\u0026rsquo;ve gotten all the way to 8110 - the loop is done, we close the file, and the script stops.\n Save this file as urls.py in your directory war-diaries. The .py reminds us that to run this file we\u0026rsquo;ll need to invoke it with python.\n  At the command prompt / terminal (windows: ananconda powershell remember!) make sure you are in your war-diaries directory with pwd to see where you are, and cd as appropriate to get to where you need to be.\n  Make sure the file is there: type ls or dir as appropriate and make sure you see the file. Let\u0026rsquo;s run this file: $ python urls.py. After a brief pause, you should just be presented with a new prompt, as if nothing has happened: but check your directory (ls or dir) and you\u0026rsquo;ll see a new file: urls.txt. You can open this with Sublime Text to see what\u0026rsquo;s inside.\n  Now that you\u0026rsquo;ve got the urls, use wget as you did for Laura Gamble\u0026rsquo;s diary (it\u0026rsquo;s the exact same command). It might go rather slowly, but keep an eye on your file explorer or finder. What have you got in your directory now?\n  Be a good digital citizen Always use the wait and limit-rate flags so that you do not overwhelm the server (the computer at the address of your url) with your requests. You can get yourself into trouble if you don\u0026rsquo;t. Make sure you understand the ideas around recursively following links, link depth, and \u0026lsquo;no-parents.\u0026rsquo; Read the original tutorials by Ian Milligan at The Programming Historian, and Kellen Kurschinski for more details.\n","description":"instructions","id":12,"section":"week","tags":null,"title":"Wget","uri":"https://craftingdh.netlify.com/week/2/wget/"},{"content":"When you look at the correspondence.csv you created, you\u0026rsquo;ll see that some names appear multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names in the original document as similar, but not the same, in the file. For example the recipient \u0026ldquo;Juan de Dios Cafiedo\u0026rdquo; is occasionally listed as \u0026ldquo;Juan de Dios CaAedo\u0026rdquo;. Any subsequent analysis will need these errors to be cleared up, and OpenRefine is a tool that will help us fix them.\nIf you had trouble with the regex and didn\u0026rsquo;t quite get to a finished correspondence.csv, that\u0026rsquo;s ok. I will share a copy with anyone who needs it to give this exercise a try.\r In this exercise, we are going to use the Open Refine tool that originated with Google. Since 2012, it has been open source and freely available online. Using it takes a bit of getting used to, however.\nInstallation Start by doing the following:\n Visit the Open Refine home page and watch the three videos. Download Open Refine 3.3 to your machine. Follow the installation instructions. Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to http://127.0.0.1:3333. This location is your own computer, so even though it looks like it’s running on the internet, it isn’t. The 3333 is a ‘port’, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser. (If the browser window doesn\u0026rsquo;t open automatically, open one and put http://127.0.0.1:3333 in the address bar.)  Cleaning and reconciling names  Make sure Open Refine is running in your browser. Start a new project by clicking on the ‘Create project’ tab on the left side of the screen. Click on ‘Choose files’ and select the Texan correspondence CSV file. Open Refine will load this data and it will give you a preview of your data. Name your project in the box on the top right side (eg. \u0026lsquo;texasnames\u0026rsquo; or similar) and then click ‘Create project’. It may take a few minutes. Once your project has started, one of the columns that should be visible in your data is \u0026ldquo;Sender\u0026rdquo;. Click on the arrow to the left of \u0026ldquo;Sender\u0026rdquo; in OpenRefine and select Facet -\u0026gt; Text Facet. Do the same with the arrow next to \u0026ldquo;Recipient\u0026rdquo;. A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. Within the \u0026ldquo;Sender\u0026rdquo; facet box on the left side, click on the button labeled \u0026ldquo;Cluster\u0026rdquo;. This feature presents various automatic ways of merging values that appear to be the same. Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. If you see two values which look like they should be merged, e.g. Ashbel Smith and . Ashbel Smith, check the box to the right in the \u0026lsquo;Merge\u0026rsquo; column and click the \u0026lsquo;Merge Selected \u0026amp; Re-Cluster\u0026rsquo; button below. Go through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. Juan de Dios CaAedo clearly should be merged with Juan de Dios Cafiedo, however Correspondent in Tampico probably should not be merged with Correspondent at Vera Cruz. Since we are not experts, we will have to use our best judgement in these cases — or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160. To finish the automatic cleaning of the data, click the arrow next to \u0026ldquo;Sender\u0026rdquo; and select \u0026lsquo;Edit Cells -\u0026gt; Common transforms -\u0026gt; Trim leading and trailing whitespace\u0026rsquo;.\nRepeat step 12 for \u0026ldquo;Recipient\u0026rdquo;. The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step.\nClick on ‘Export’ at the top right of the window to get your data back out as a .csv file.  Ta da! A file ready for some network analysis.\n   Turn on the closed captions; the video is from an earlier iteration of this course, but takes you through those steps.\n","description":"Basic Tools","id":13,"section":"week","tags":null,"title":"Open Refine","uri":"https://craftingdh.netlify.com/week/3/open-refine/"},{"content":"Follow this tutorial by Heather Froelich to install and use AntConc, but give it a try on this corpus of historical documents I have prepared for you. This corpus comes from the National Library of Scotland collection of chapbooks printed in Scotland. The data is the OCR\u0026rsquo;d text from more than 3000 chapbooks, mostly from the 18th and 19th centuries. A chapbook was a cheaply printed booklet, and dealt with popular subjects. Think of them as the checkout aisle magazines of their day.\nThe data is zipped; unzip it and then you can load it into AntConc. This file contains the metadata for the chapbooks. You could use this to identify the filenames of particular kinds of works you might be interested in, and then copy just those files into a new directory for exploration with AntConc.\nTo give you an idea of how rich an analysis with AntConc could be, check out this example concerning 1990s juvenile literature in ways that would be completely comparable for eg scottish chapbooks database https://datasittersclub.github.io/site/dsc4/)\nLang, Anouk. \u0026ldquo;DSC #4: AntConc Saves the Day.\u0026rdquo;\u0026rdquo; The Data-Sitters Club. April 10, 2020. https://datasittersclub.github.io/site/dsc4/.\nGoing Further You can try to perform some of the same kinds of analysis in Python; jump to step 2.1 and follow along for more analysis on materials from the National Library of Scotland.\nJupyter Notebook by Beatrice Alex.\n","description":"Of Macroscopes and Microscopes","id":14,"section":"week","tags":null,"title":"AntConc","uri":"https://craftingdh.netlify.com/week/4/antconc/"},{"content":"Bringing It All Together Goals for this week  experience something of the cycle of digital history by trying some exploratory analysis with the tools/approaches you\u0026rsquo;ve learned  Listen Read  Nothing formal from me to read this week.  Do Remember back in week one, how I had you explore that twitter thread by Amalia Skarlatou Levi? This week, you\u0026rsquo;ll be selecting one of those resources she shared (or, if none strike your fancy, something from the resources gathered here) to do some exploratory digital history.\nYou will use at least three appropriate techniques from weeks 1 to 4 to identify and surface some interesting historical observations from those sources, and you will imagine the possible directions such research might take.\nYou will share your results using an appropriate technique you encountered in week 5. Other possibilities are ok too; just check with Dr. Graham first. Remember, you don\u0026rsquo;t have a lot of time, so keep your ambitions in check: the perfect is the enemy of the good.\n When/if you run into trouble, take screenshots (google how to do that for your particular machine) and these can be uploaded into your repository as well. Indeed, you should also keep track of any files you create as part of your weekly work in your repo: these are evidence!  With tech work, if it doesn\u0026rsquo;t come together in about 30 minutes, it won\u0026rsquo;t come in an hour. So take a break. Close the laptop. Call somebody up for help. Find another pair of eyes to look at the problem. I don\u0026rsquo;t want to hear that you labored heroically for 2 hours to do something. Jump into our social space and ask for advice. Remember: it\u0026rsquo;s not how many you do, it\u0026rsquo;s that you pushed yourself that matters.\r Record and Reflect   As you have been doing, make another notes.md entry and put it in your repo for week 6.\n  In your journal.md for this week, share the results of your explorations. Also, think about this: Digital history is a process of encountering a body of information that suggests a question or two to you and then cycling back to that body of information to better understand the question (we haven\u0026rsquo;t even got to an answer yet!). In so doing, you\u0026rsquo;ll discover that the information is messy, or in the not-quite-right format. 80% of our time, as digital historians, is just getting things into shape for us to begin our exploration!\n  Submit work You can submit the link to your work on this form\n","description":"Bringing it all Together","id":15,"section":"week","tags":null,"title":"Instructions: June 8","uri":"https://craftingdh.netlify.com/week/6/instructions/"},{"content":" storymap leaflet webmapping != GIS, pointers to QGIS  ","description":"Telling the Compelling Story","id":16,"section":"week","tags":null,"title":"Mapping","uri":"https://craftingdh.netlify.com/week/5/mapping/"},{"content":"Sometimes, a website will have what is called an Application Programming Interface or API. In essence, this lets a program on your computer talk to the computer serving the website you\u0026rsquo;re interested in, such that the website gives you the data that you\u0026rsquo;re looking for.\nAn API is just a way of opening up a program so that another program can interact with it. That is, instead of an interface meant for a human to interact with the machine, there’s an API to allow some other machine to interact with this one. If you’ve ever downloaded an app on your phone that allowed you to interact with Instagram (but wasn’t Instagram itself), that interaction was through Instagram’s API, for instance.\nA good API will also have documentation explaining what or how to make calls to it to get the information you want. That is, instead of you punching in the search terms on a search page, and copying and pasting the results, you frame a request as a URL. More or less. The results often come back to you in a text format where data is organized according to keys and values (JSON). Sometimes it\u0026rsquo;s just a table of data using commas to separate each field for each row (.csv file). JSON looks like the following:\nIn what follows, push yourself until you get stuck. I\u0026rsquo;m not interested in how far you get, but rather in how you document what you are able to do, how you look for help, how you reach out to others - or how you help others over the bumps. I know also that you all have lots of other claims on your time. Reading through all of this and making notes on what you do/don\u0026rsquo;t understand is fine too.\r Getting material out of an API Each API has its own idiosyncracies, but we can always look at the documentation and figure out how to form our requests so that our programs grab the data we\u0026rsquo;re after. The Chronicling America website from the Library of Congress has digitized American newspapers from 1789 to 1963. While the site has a fine search interface, we\u0026rsquo;ll use the API to grab every article that mentions the word \u0026lsquo;archeology\u0026rsquo; (sic).\nFirst of all, go to the Cronicling America site and search in the text box for archeology. Notice how the url changes when it brings back the results:\nhttps://chroniclingamerica.loc.gov/search/pages/results/?state=\u0026amp;date1=1789\u0026amp;date2=1963\u0026amp;proxtext=archeology\u0026amp;x=0\u0026amp;y=0\u0026amp;dateFilterType=yearRange\u0026amp;rows=20\u0026amp;searchType=basic\nThere\u0026rsquo;s a lot of stuff in there - amongst other things, we can see a setting for state, for date1 and date2, our search term appears as the value for a setting called proxtext. This is the API in action.\nLet\u0026rsquo;s build.\n Open a new file in Sublime Text. We\u0026rsquo;ll first put a bit of metadata in our file, for the programme we\u0026rsquo;re going to write:  1 2 3 4 5 6 7 8 9 10  #!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; a script for getting materials from the Chronicling America website \u0026#34;\u0026#34;\u0026#34; # Make these modules available import requests import json __author__ = \u0026#34;your-name\u0026#34;   The first bit tells us this is a python file. The next bit tells us what the file is for. The import tells python that we\u0026rsquo;ll need a module called requests which lets us grab materials from the web, and json which helps us deal with json formatted data. The final bit says who wrote the script.\nNow we\u0026rsquo;re going to define some variables to hold the bit of the search url up to where the ? occurs - everything after the question mark are the parameters we want the API to search. We create the api_searh_url, define the parameter we want to search for, and define how we want the results returned to us. Add the following to your script:  1 2 3 4 5 6 7 8 9 10  # Create a variable called \u0026#39;api_search_url\u0026#39; and give it a value api_search_url = \u0026#39;https://chroniclingamerica.loc.gov/search/pages/results/\u0026#39; # This creates a dictionary called \u0026#39;params\u0026#39; and sets values for the API\u0026#39;s mandatory parameters params = { \u0026#39;proxtext\u0026#39;: \u0026#39;archeology\u0026#39; # Search for this keyword  } # This adds a value for \u0026#39;encoding\u0026#39; to our dictionary params[\u0026#39;format\u0026#39;] = \u0026#39;json\u0026#39;   Now we\u0026rsquo;ll send the request to the server, and we\u0026rsquo;ll add a bit of error checking so that if something is wrong, we\u0026rsquo;ll get some indication of why that is. Add this to your script:  1 2 3 4 5 6 7 8 9 10 11 12 13  # This sends our request to the API and stores the result in a variable called \u0026#39;response\u0026#39; response = requests.get(api_search_url, params=params) # This shows us the url that\u0026#39;s sent to the API print(\u0026#39;Here\\\u0026#39;s the formatted url that gets sent to the ChronAmerca API:\\n{}\\n\u0026#39;.format(response.url)) # This checks the status code of the response to make sure there were no errors if response.status_code == requests.codes.ok: print(\u0026#39;All ok\u0026#39;) elif response.status_code == 403: print(\u0026#39;There was an authentication error. Did you paste your API above?\u0026#39;) else: print(\u0026#39;There was a problem. Error code: {}\u0026#39;.format(response.status_code))   Now let\u0026rsquo;s get the results, and put them into a variable called \u0026lsquo;data\u0026rsquo;. Then we\u0026rsquo;ll print the results to the terminal, and finish by also writing the results to a file.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # Get the API\u0026#39;s JSON results and make them available as a Python variable called \u0026#39;data\u0026#39; data = response.json() # Let\u0026#39;s prettify the raw JSON data and then display it. # We\u0026#39;re using the Pygments library to add some colour to the output, so we need to import it from pygments import highlight, lexers, formatters # This uses Python\u0026#39;s JSON module to output the results as nicely indented text formatted_data = json.dumps(data, indent=2) # This colours the text highlighted_data = highlight(formatted_data, lexers.JsonLexer(), formatters.TerminalFormatter()) # And now display the results print(highlighted_data) # dump json to file with open(\u0026#39;data.json\u0026#39;, \u0026#39;w\u0026#39;) as outfile: json.dump(data, outfile)   Save your file as ca.py. Open a terminal/command prompt (remember Windows folks: anaconda powershell!) in the folder where you saved this file, and run it with:\n$ python ca.py\nYour terminal will look like it\u0026rsquo;s frozen for a few moments; that\u0026rsquo;s because your computer is reaching out to the Chronicling America website, making its request, and pulling down the results. But in seconds, you\u0026rsquo;ll have a data.json file with loads of data - 9021 articles in fact!\nCongratulations, you now have a program that you wrote that you can use to obtain all sorts of historical information.\nYour complete file will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  #!/usr/bin/env python \u0026#34;\u0026#34;\u0026#34; a script for getting materials from the Chronicling America website \u0026#34;\u0026#34;\u0026#34; # Make these modules available import requests import json __author__ = \u0026#34;your-name\u0026#34; # Create a variable called \u0026#39;api_search_url\u0026#39; and give it a value api_search_url = \u0026#39;https://chroniclingamerica.loc.gov/search/pages/results/\u0026#39; # This creates a dictionary called \u0026#39;params\u0026#39; and sets values for the API\u0026#39;s mandatory parameters params = { \u0026#39;proxtext\u0026#39;: \u0026#39;archeology\u0026#39; # Search for this keyword } # This adds a value for \u0026#39;encoding\u0026#39; to our dictionary params[\u0026#39;format\u0026#39;] = \u0026#39;json\u0026#39; # This sends our request to the API and stores the result in a variable called \u0026#39;response\u0026#39; response = requests.get(api_search_url, params=params) # This shows us the url that\u0026#39;s sent to the API print(\u0026#39;Here\\\u0026#39;s the formatted url that gets sent to the ChronAmerca API:\\n{}\\n\u0026#39;.format(response.url)) # This checks the status code of the response to make sure there were no errors if response.status_code == requests.codes.ok: print(\u0026#39;All ok\u0026#39;) elif response.status_code == 403: print(\u0026#39;There was an authentication error. Did you paste your API above?\u0026#39;) else: print(\u0026#39;There was a problem. Error code: {}\u0026#39;.format(response.status_code)) # Get the API\u0026#39;s JSON results and make them available as a Python variable called \u0026#39;data\u0026#39; data = response.json() # Let\u0026#39;s prettify the raw JSON data and then display it. # We\u0026#39;re using the Pygments library to add some colour to the output, so we need to import it from pygments import highlight, lexers, formatters # This uses Python\u0026#39;s JSON module to output the results as nicely indented text formatted_data = json.dumps(data, indent=2) # This colours the text highlighted_data = highlight(formatted_data, lexers.JsonLexer(), formatters.TerminalFormatter()) # And now display the results print(highlighted_data) # dump json to file with open(\u0026#39;data.json\u0026#39;, \u0026#39;w\u0026#39;) as outfile: json.dump(data, outfile)   Some other APIs You can modify this code to extract information from other APIs, but it takes a bit of tinkering. In essence, you need to study the website to see how they form the API, and then change up lines 13, 17 and 21 accordingly. You can see this in action for instance here, with regard to the Metropolitan Museum of Art or here, with regard to the Smithsonian. My Winter 2020 digital museums\u0026rsquo; class made APIs from collections at some of our national museums; you can see a modified version of the code to access some of their work here\nBut\u0026hellip; it\u0026rsquo;s in json format? JSON is handy for lots of computational tasks, but for you as a beginning digital historian, you might want to have the data as a table. There are a couple of options here. The easiest right now - and there\u0026rsquo;s no shame in doing this - is to use an online converter. This site: json-csv.com lets you convert your json file to csv or Excel spreadsheet, and even transfer it over to a google doc. Give that a shot right now; the text of the articles by the way is in the field \u0026lsquo;ocr_eng\u0026rsquo; which tells us that the text was originally transcribed from the images using object character recognition - so there will be errors and weird glitches in the text. Fortunately, there\u0026rsquo;s also a URL with the direct link to the original document, so you can check things for yourself.\nGLAM Workbench \u0026lsquo;GLAM\u0026rsquo; stands for \u0026lsquo;galleries, libraries, archives, and museums\u0026rsquo;. The GLAM Workbench is by Tim Sherratt, a digital historian in Australia. I would strongly recommend that you explore and give the Workbench a whirl if you are at all interested in the kinds of work that you might be able to do when you are computationally able to treat collections as data. For a glimpse as to what that might mean, check out this presentation.\n","description":"instructions","id":17,"section":"week","tags":null,"title":"APIs","uri":"https://craftingdh.netlify.com/week/2/apis/"},{"content":"Zotero is a piece of software for managing your research. When it is installed, it can extract metadata from websites or pdfs (when these have metadata) and store the information in your very own library. Zotero can be connected to Word or Google Docs so that when it is time to insert a reference, you can select the reference from your library - and then get the machine to automatically format/update your bibliography against whatever citation style you use.\nA tutorial on how to get, how to install, and how to use Zotero to keep track of what you read. Right-click and open in a new window; this tutorial from the UCLA Library will also show you how to use Zotero effectively.\nYou can download Zotero here. Note that you have to also install \u0026lsquo;connectors\u0026rsquo; for your browser.\nThe installation instructions for Zotero are here.\n","description":"instructions","id":18,"section":"week","tags":null,"title":"Setting up your Zotero","uri":"https://craftingdh.netlify.com/week/1/zotero/"},{"content":"Recall that the index of the collected letters of the Republic of Texas was just a list of letters from so-and-so to so-and-so. We haven\u0026rsquo;t looked at the content of those letters, but the shape of network — the meta data of that correspondence — can be revealing (remember Paul Revere!) When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. I would recommend that you also take a long look at Scott Weingart\u0026rsquo;s series, Networks Demystified.\nIf you had trouble with the regex and didn\u0026rsquo;t quite get Open Refine to work, that\u0026rsquo;s ok. You can use this file for this gentle introduction to networks. Right-click and save-as that link.\r When Not To Use Networks Networks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for multimodal networks, but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question.\nNetworks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself.\nWhen deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.\nDatabasic.io At Databasic.io there are simple web-tools to help you see some large-scale patterns in your information. Let\u0026rsquo;s use Connect the Dots which is a simple network visualizer and analyzer - it will find communities of nodes (dots; people, in our data) and it will give us two network metrics for each node - the degree (number of connections it has) and the centrality (here, imagined as being a person who is on lots of paths connecting all other pairs).\n Go to [Connect the Dots](https://databasic.io/en/connectthedots/#upload and upload your Texas Correspondence file. Make sure it has a first row with source,target and that there are no spaces on either side of that comma, eg:  source,target Sam Houston,J. Pinckney Henderson James Webb,Alc6e La Branche\nBoom! A network visualization! What do you observe? (Scroll down to the \u0026lsquo;so what?').  More complex visualizatons and analysis Connect the dots is pretty basic. If you want to do more complex network visualizations and analysis, you can try Gephi:\n Download and install Gephi. Open Gephi by double-clicking its icon. Click \u0026ldquo;New project\u0026rdquo;. The middle pane of the interface window is the “Data Laboratory,” where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in Open Refine. In the Data Laboratory, select “Import Spreadsheet.” Press the ellipsis ... and locate the CSV you created. Make sure that the Separator is listed as Comma and the As table is listed as Edges table. Press Next then Finish. Your data should load up. Click on the \u0026ldquo;Overview” tab and you will be presented with a tangled network graph.  Navigating Gephi Gephi is broken up into three panes: Overview, Data Laboratory, and Preview.\n The Overview pane is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. The Data Laboratory is for adding, manipulating, and removing data. Use the Preview pane to do some final tweaks on the look and feel of the network and to export an image for publication.  There is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi.\n Click on the Data Laboratory tab. Click on the “Nodes” tab in the Data Table (this should be open already) and notice that, of the three columns, “Label” (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful. In the “Nodes” tab, click “Copy data to other column” at the bottom, select “ID”, and press “Ok”. Upon doing so, the “Label” column will be filled with the appropriate labels for each correspondent. While you’re still in the Data Laboratory, look in the “Edges” tab and notice there is a “Weight” column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the “Weight.” This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the “Source” column, Webb in the “Target”, and the “Weight” is three. Clicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the “Graph” tab. The “Context” tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense.  There is a video from an earlier iteration of the course here walking through these steps. Make sure to turn on the closed captions).\n Fix the nodes by selecting a layout in the “Layout” tab – the best one for beginners is “Force Atlas 2.”\n  Press the “Run” button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled “Stop”) to settle the nodes in their place.\n  You just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction).\nAbout two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network.\n The first step is to calculate which components of the network are connected to which others; do this by clicking “Run” next to the text that says “Connected Components” in the “Statistics” tab on the right side.\n  Once there, select “UnDirected” and press “OK.”\n  Press “Close” when the report pops up indicating that the algorithm has finished running. Now that this is done, Gephi knows which is the giant connected component and has labeled that component “0”.\n  To filter out everything but the giant component, click on the “Filters” tab on the right side and browse to \u0026ldquo;Component ID Integer (Node)\u0026rdquo; in the folder directory (you’ll find it under \u0026ldquo;Attributes,\u0026rdquo; then \u0026ldquo;Equal\u0026rdquo;).\n  Double-click \u0026ldquo;Component ID Integer (Node)\u0026rdquo; and click the \u0026ldquo;Filter\u0026rdquo; button at the bottom. Doing this removes the disconnected bundles of nodes.\n  There are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of each node in the network. This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent’s importance in the Republic of Texas letter network.\n Calculate the PageRank by clicking on the \u0026ldquo;Run\u0026rdquo; button next to \u0026ldquo;PageRank\u0026rdquo; in the \u0026ldquo;Statistics\u0026rdquo; tab. You will be presented with a prompt asking for a few parameters; make sure \u0026ldquo;Directed\u0026rdquo; network is selected and that the algorithm is taking edge weight into account (by selecting \u0026ldquo;Use edge weight\u0026rdquo;). Leave all other parameters at their default.\n  Press \u0026ldquo;OK\u0026rdquo;.\n  Once PageRank is calculated, if you click back into the \u0026ldquo;Data Laboratory\u0026rdquo; and select the \u0026ldquo;Nodes\u0026rdquo; list in the Data Table, you can see that a new \u0026ldquo;PageRank\u0026rdquo; column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network.\n  There is a video from an earlier iteration of the course here walking through these steps. Make sure to turn on the closed captions. You probably are sick of the music, so mute that.\n Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent’s node based on its PageRank. Do this in the \u0026ldquo;Ranking\u0026rdquo; tab on the left side of the Overview pane.\n  Make sure \u0026ldquo;Nodes\u0026rdquo; is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu.\n  In the parameter options just below, enter the \u0026ldquo;Min size\u0026rdquo; as 1 and the \u0026ldquo;Max size\u0026rdquo; as 10.\n  Press \u0026ldquo;Apply,\u0026rdquo; and watch the nodes resize based on their PageRank.\n  To be on the safe side and decrease clutter, re-run the \u0026ldquo;Force Atlas 2\u0026rdquo; layout as described above, making sure to keep the \u0026ldquo;Prevent Overlap\u0026rdquo; box checked.\n  Another video here covering these last few steps. I\u0026rsquo;m really sorry about the music.\nAt this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data.\n In Preview, on the left side, select \u0026ldquo;Show Labels,\u0026rdquo; \u0026ldquo;Proportional Size,\u0026rdquo; \u0026ldquo;Rescale Weight,\u0026rdquo; and deselect \u0026ldquo;Curved\u0026rdquo; edges.\n  Press \u0026ldquo;Refresh.\u0026rdquo;\n  Phew. Save your work. Gephi saves as .gephi file, but you can also export to other common formats like .gexf\nNetwork Analysis in Python You could try this tutorial from the Programming Historian out.\nSo what? So what have we got?\nThe visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network?\n","description":"Basic Tools","id":19,"section":"week","tags":null,"title":"Networks","uri":"https://craftingdh.netlify.com/week/3/networks/"},{"content":"An Introduction to Topic Models   that link at the end, if you\u0026rsquo;re interested: http://bit.ly/sw-tm-tour\nTopic Modeling with the TMTool There are many tools that can fit a topic model to your text. The easiest one to use is a graphical user interface overlaid on the MALLET Toolkit called \u0026lsquo;The Topic Modeling Tool\u0026rsquo; by Jonathan Enderle.\n Follow Enderle\u0026rsquo;s instructions for downloading and installing the tool here Make a new directory on your machine called tmt. Make a subdirectory in tmt called input. Make a new subdirectory in tmt and call it output. Download, unzip, and then copy the text files from the Scottish Chapbooks dataset into your input directory. Start up the Topic Modeling Tool; you\u0026rsquo;ll see this interface:\n Click on and select your input folder but only click once. If you double click, you go into the folder, and that\u0026rsquo;s not what you want. Select where you want the output from the process to go. Click on the output button, and select your output directory. Decide on how many topics you want to look for, and then click \u0026lsquo;learn topics\u0026rsquo;\n  And soon the console will load up with results. If you watch carefully, you\u0026rsquo;ll see the console build up the original command that it feeds to MALLET (and which, if you were working with MALLET at the command line, you\u0026rsquo;d have to put together yourself). It will run through several iterations (which you can tweak on the \u0026lsquo;optional settings\u0026rsquo; button) to try to find the best fit of your number of topics to the data.\nBut what does it mean? Ah, well\u0026hellip; for that, please read \u0026lsquo;Very Basic Strategies for Interpreting Results from the Topic Modeling Tool\u0026rsquo; by Andy Wallace; follow that up Enderle\u0026rsquo;s section on analyzing the output from his quickstart guide (scroll down).\nClick on your output folder, and then the output_html folder you\u0026rsquo;ll find inside. Click on the index.html; this will open in your browser, and it will show you the topics it found as lists of each topic\u0026rsquo;s keywords; click on a topic, and it\u0026rsquo;ll show you the relevant documents; click on a document and it\u0026rsquo;ll give you a text snippet for that document, plus clickable links to the other documents present in that document. It\u0026rsquo;s a bit of a topic browser, and lets you cycle from distant to close and back again as you explore your results.\nA bit of visualization is always useful though. Try making a pivot table, as Enderle describes. Make some charts, as Wallace describes. What decisions do you have to make, to make sense of the materials? What other data might you need in order to figure out what these patterns might mean?\nTopic Model in R The Topic Model Tool is an excellent tool for building and exploring topic models. But you might want to try building a topic model in R. R is a powerful language for statistical exploring, visualizing, and manipulating all kinds of data, including textual. While R can be worked with from the command line, RStudio is what we need to preserve our sanity. RStudio can be started up from your Anaconda Navigator - you met RStudio back in week 2 remember.\nR allows us to do scripted analyses. We write out the sequence of transformations or analyses to be done, making a kind of program that we can then feed our data into. Once we have a workflow that does what we need it to do, it becomes trivial to re-use the code on other datasets. What\u0026rsquo;s more, when we do an analysis, we can publish the scripts and the data. We don\u0026rsquo;t have to taken an author\u0026rsquo;s word for it: we can re-run the analysis ourselves or build upon it.\nThis course only touches in the lightest manner on the potential for R for doing digital history. Please see Lincoln Mullen\u0026rsquo;s Computational Historical Thinking with Applications in R for more instruction if you\u0026rsquo;re interested.\n","description":"Of Macroscopes and Microscopes","id":20,"section":"week","tags":null,"title":"Topic Models","uri":"https://craftingdh.netlify.com/week/4/topic-models/"},{"content":" academic posters infographics  colour theory, layout, stuff from macroscope re scott chapter\n","description":"Telling the Compelling Story","id":21,"section":"week","tags":null,"title":"Infographics","uri":"https://craftingdh.netlify.com/week/5/infographics/"},{"content":"What is Discord? You can download Discord here or you can use it in a broswer.\nYou will receive an invitation email in your Carleton account. It expires after one day, so do join once you get it. It will ask to verify your email; check your spam folder.\nOnce you\u0026rsquo;re in, there\u0026rsquo;s a welcome message with a reaction emoji at the end - click on the thumbs up to confirm that you\u0026rsquo;ve read the message. This will unlock the various channels in the server, including voice and screensharing.\n(Alex is a student who works for me on the bone trade project if you were wondering.)\nYou can customize your profile by clicking on the cogwheel icon at the bottom of the navigation:\nand you can customize how you\u0026rsquo;d like to use voice and video:\nThere\u0026rsquo;s a channel for each week\u0026rsquo;s materials, a channel for general history or Carleton chat, and a social channel just for shooting the breeze. If you want to chat or stream your machine (eg to show us what you\u0026rsquo;re trying to do and how it\u0026rsquo;s not working) just click on one of the side-chats - make sure you\u0026rsquo;re unmuted! Dr. Graham also has a private office for voide and video and you can ask him directly to speak there.\nWhat all the buttons do help file\nDiscord has accessibility features, more details here\n","description":"instructions","id":22,"section":"week","tags":null,"title":"Getting started with Discord","uri":"https://craftingdh.netlify.com/week/1/discord/"},{"content":"Object Character Recognition is a technique that looks at the pattern of light and dark pixels in an image and matches them against the alphabet. The technique was developed against clean type-written pages where the letters were crisply made. On historical documents, things can get pretty hit-and-miss. When we searched the Chronicling America website, we were actually searching the OCR\u0026rsquo;d text that was embedded in each of those pdfs (if you can select text in a pdf, it has a hidden text layer on top of the image. If you can\u0026rsquo;t, it\u0026rsquo;s just an image). We probably missed a lot of information because we searched for \u0026lsquo;archeology\u0026rsquo; but it\u0026rsquo;s entirely possible the word got transcribed as \u0026lsquo;arc50log\u0026rsquo; or other similar mismashes. Just because you searched, didn\u0026rsquo;t mean you found what was there!\nIn this walk through, we\u0026rsquo;re going to use the R language to run some images through the OCR process. I am not teaching you R, but rather, giving you two recipes that you can use.\nIn what follows, push yourself until you get stuck. I\u0026rsquo;m not interested in how far you get, but rather in how you document what you are able to do, how you look for help, how you reach out to others - or how you help others over the bumps. I know also that you all have lots of other claims on your time. Reading through all of this and making notes on what you do/don\u0026rsquo;t understand is fine too.\r One file at a time  Open Anaconda Navigator, and hit the \u0026lsquo;Install R Studio\u0026rsquo; button.\n Launch R Studio\n RStudio has within it a console, a script editor, a file explorer, and lots of other features. Click on the new button to make a new R Script.\n  We are going to OCR this image, from the 14th Canadian General Hospital war diaries:\n Right-click on that image, \u0026lsquo;save-as\u0026rsquo;, and save it somewhere sensible on your computer.\n  Using the file explorer in R Studio, navigate to where you saved that file.\n  Make that folder your \u0026lsquo;working directory\u0026rsquo;; you can do this by clicking on the \u0026lsquo;More\u0026rsquo; button beside the cogwheel. When you do this, the actual R command that achieves this will also copy into the console. R Studio now knows where to look for files, and where to save them.\n  In the script window, paste the following lines:  1 2 3  # install only the first time install.packages(\u0026#39;magick\u0026#39;) install.packages(\u0026#39;tesseract\u0026#39;)   We\u0026rsquo;ll only run these lines once, because once we\u0026rsquo;ve installed these bits of lego, we won\u0026rsquo;t have to install them again; they\u0026rsquo;ll always be available for use. Highlight all of them with your cursor, and hit the \u0026lsquo;run\u0026rsquo; button at the top of the window. Running the lines passes them to the console where they are executed; you\u0026rsquo;ll see a bunch of information scroll by as they install. Then, when it\u0026rsquo;s finished, you\u0026rsquo;ll see the \u0026gt; prompt again.\nAdd to your script these lines:  1 2 3 4  # tell R which packages you need library(magick) library(magrittr) library(tesseract)   And run them. Down in the console, nothing much should happen, but you should get the \u0026gt; again.\nAnd now let\u0026rsquo;s ocr some text. Add these lines to your script:  1 2 3 4 5 6 7 8 9 10 11 12 13  # now let\u0026#39;s create a variable called \u0026#39;text\u0026#39; # and read the image into it # and we\u0026#39;ll modify the image to give us # the best chance of reading the text # the final line, image_ocr() does the text extraction # remember to set your working directory to whatever folder # contains the stuff you want to work on. text \u0026lt;- image_read(\u0026#34;e001518030.jpg\u0026#34;) %\u0026gt;% image_resize(\u0026#34;2000\u0026#34;) %\u0026gt;% image_convert(colorspace = \u0026#39;gray\u0026#39;) %\u0026gt;% image_trim() %\u0026gt;% image_ocr()   If you put your cursor at the \u0026lsquo;t\u0026rsquo; in \u0026lsquo;text\u0026rsquo;, and hit \u0026lsquo;run\u0026rsquo;, R Studio knows that all of these lines are joined by the %\u0026gt;% function, or pipe, and so will run everything in order to the final command, image_ocr().\nYou know it ran, because it passed the commands to the console, and you got the command prompt back.\nFinally, let\u0026rsquo;s see what we\u0026rsquo;ve got! Add the following line:  write.table(text, \u0026quot;output.txt\u0026quot;)\nand run it.\nSave your R script as \u0026ldquo;one-image-ocr.R\u0026rdquo;.\nLooping over many files at once  Grab a handful of images from the war diary and save them inside the folder you are working in, in a new subfolder (three or four are fine; I just right-clicked and save image on the urls at http://data2.archives.ca/e/e061/e001518034.jpg http://data2.archives.ca/e/e061/e001518035.jpg etc for the purposes of this recipe).  Now, we\u0026rsquo;re going to modify the first recipe to include a loop, and to apply the image modifications and to do the ocr to each image in turn. Make a new script, and then put into it which libraries you\u0026rsquo;re going to use. Run those lines.  1 2 3 4  # tell R which packages you need library(magick) library(magrittr) library(tesseract)   I put the files I wanted to work on into a subfolder called \u0026lsquo;many-pics\u0026rsquo;. Now we create a variable called \u0026lsquo;myfiles\u0026rsquo; into which we\u0026rsquo;re going to put the list of files held in our directory of files.  1 2  imgsource \u0026lt;- \u0026#34;many-pics\u0026#34; myfiles \u0026lt;- list.files(path = imgsource, pattern = \u0026#34;jpg\u0026#34;, full.names = TRUE)   Put these lines into your script, then run them. Over in the environment pane, you\u0026rsquo;ll see them get created, and you\u0026rsquo;ll see what\u0026rsquo;s inside them.\nNow the loop. Copy this into your script:  1 2 3 4 5 6 7 8 9 10  lapply(myfiles, function(i){ text \u0026lt;- image_read(i) %\u0026gt;% image_resize(\u0026#34;3000x\u0026#34;) %\u0026gt;% image_convert(type = \u0026#39;Grayscale\u0026#39;) %\u0026gt;% image_trim(fuzz = 40) %\u0026gt;% image_write(format = \u0026#39;png\u0026#39;, density = \u0026#39;300x300\u0026#39;) %\u0026gt;% tesseract::ocr() outfile \u0026lt;- paste(i,\u0026#34;-ocr.txt\u0026#34;,sep=\u0026#34;\u0026#34;) cat(text, file=outfile, sep=\u0026#34;\\n\u0026#34;)   Techically, this isn\u0026rsquo;t actually a loop. We\u0026rsquo;re just applying all of this image fixing and text extraction to each file in our myfiles variable, and then creating a unique text file appending -ocr.txt to the original filename. Put the cursor at the l in lapply, and then hit run; the machine will take some time to do its thing, but then\u0026hellip;\nNow imagine how much you\u0026rsquo;re missing when you do keyword searches of OCR\u0026rsquo;d documents\u0026hellip;\nSave your work.\nWhat about handwriting? Handwriting is a much more complicated problem, but in recent years machine learning has made enormous strides in this regard. I will point you to some documents I prepared for another class of mine if you want to explore this problem.\n\u0026lsquo;Detecting and Transcribing Handwriting with Microsoft Azure\u0026rsquo;\n","description":"instructions","id":23,"section":"week","tags":null,"title":"OCR","uri":"https://craftingdh.netlify.com/week/2/ocr/"},{"content":"If you\u0026rsquo;re interested in other network tools or data cleaning tools, drop me a line. Many folks use R and R Studio and its various network analysis packages, which have the edge over Gephi in that we can write the script and share that, rather than long, tortured step-by-step \u0026lsquo;click this\u0026rsquo; and \u0026lsquo;click that\u0026rsquo; type instructions.\nAh, what the hang, here we go.\nGrab this data:\nList of links (this downloads a CSV file)\nList of nodes(this downloads a CSV file)\nFire up R Studio, make a new script, run each line one at a time:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  # install igraph; this might take a long time # you only run this line the first time you install igraph: install.packages(\u0026#39;igraph\u0026#39;) # a lot of stuff gets downloaded and installed. # now tell RStudio you want to use the igraph pacakge and its functions: library(\u0026#39;igraph\u0026#39;) # now let\u0026#39;s load up the data by putting the csv files into nodes and links. nodes \u0026lt;- read.csv(\u0026#34;texasnodes.csv\u0026#34;, header=T, as.is=T) links \u0026lt;- read.csv(\u0026#34;texaslinks.csv\u0026#34;, header=T, as.is=T) #examine data head(nodes) head(links) nrow(nodes); length(unique(nodes$id)) # which gives the number of nodes in our data nrow(links); nrow(unique(links[,c(\u0026#34;source\u0026#34;, \u0026#34;target\u0026#34;)])) # which gives the number of sources, and number of targets # which means some people sent more than one letter, and some people received more than one letter links \u0026lt;- aggregate(links[,3], links[,-3], sum) links \u0026lt;- links[order(links$target, links$source),] colnames(links)[3] \u0026lt;- \u0026#34;weight\u0026#34; rownames(links) \u0026lt;- NULL head(links) # let\u0026#39;s make a net # notice that we are telling igraph that the network is directed, that the relationship Alice to Bob is different than Bob\u0026#39;s to Alice (Alice is the _sender_, and Bob is the _receiver_) # In older DH Box version of igraph in RStudio: net \u0026lt;- graph.data.frame(d=links, vertices=nodes, directed=T) # OR Newer version of igraph in desktop RStudio: net \u0026lt;- graph_from_data_frame(d=links, vertices=nodes, directed=T) # type \u0026#39;net\u0026#39; again and run the line to see how the network is represented. net # let\u0026#39;s visualizae it plot(net, edge.arrow.size=.4,vertex.label=NA) # two quite distinct groupings, it would appear. --- Before we jump down the rabbit hole of visualization, let\u0026#39;s recognize right now that visualizing a network is only rarely of analytical value. The value of network analysis comes from the various questions we can now start posing of our data when it is represented as a network. In this correspondence network, who is in the centre of the web? To whom would information flow? To whom would information leak? Are there cliques or ingroups? When we identify such individuals, how does that confirm or confound our expectations of the period and place? Many different kinds of metrics can be calculated (and the [Kateto R igraph tutorial](https://kateto.net/networks-r-igraph) will show you how) but it\u0026#39;s always worth remembering that a metric is only meaningful for a given network when we\u0026#39;re dealing with similar things — a network of people who write letters to one another; a network of banks that swap mortgages with one another. These are called \u0026#39;one mode networks\u0026#39;. A network of people connected to the banks they use — a two mode network, because it connects two different kinds of things — might be useful to visualize but the metrics calculated might not be valid if the metric was designed to work on a one-mode network . Given our correspondence network, let\u0026#39;s imagine that \u0026#39;closeness\u0026#39; (a measure of how central a person is) and \u0026#39;betweenness\u0026#39; (a measure of how many different strands of the network pass through this person) are the most historically interesting. Further, we\u0026#39;re going to try to determine if there are subgroups in our network, cliques. ```R ## the \u0026#39;degree\u0026#39; of a node is the count of its connections. In this code chunk, we calculate degree, then make both a histogram of the counts and a plot of the network where we size the nodes proportionately to their degree. What do we learn from these two visualizations? deg \u0026lt;- degree(net, mode=\u0026#34;all\u0026#34;) hist(deg, breaks=1:vcount(net)-1, main=\u0026#34;Histogram of node degree\u0026#34;) plot(net, vertex.size=deg*2, vertex.label = NA) ## write this info to file for safekeeping write.csv(deg, \u0026#39;degree.csv\u0026#39;)   Now we\u0026rsquo;ll look at closeness. If you know the width or diameter of your network (the maximum number of steps to get across it), then the node that is on average the shortest number of steps from all the others is the one that is closest. We calculate it like the following:\n1 2 3  closepeople \u0026lt;- closeness(net, mode=\u0026#34;all\u0026#34;, weights=NA) sort(closepeople, decreasing = T) # so that we see who is most close first write.csv(closepeople, \u0026#39;closeness.csv\u0026#39;) # so we have it on file.   We can ask which individuals are hubs, and which are authorities. In the lingo, \u0026lsquo;hubs\u0026rsquo; are individuals with many outgoing links (they sent lots of letters) while \u0026lsquo;authorities\u0026rsquo; are individuals who received lots of letters. In the code below, can you work out what command to give to write the hub score or the authority scores to a file?\n1 2 3 4 5 6 7 8  hs \u0026lt;- hub_score(net, weights=NA)$vector as \u0026lt;- authority_score(net, weights=NA)$vector par(mfrow=c(1,2)) # vertex.label.cex sets the size of the label; play with the sizes until you see something appealing. plot(net, vertex.size=hs*40, vertex.label.cex =.2, edge.arrow.size=.1, main=\u0026#34;Hubs\u0026#34;) plot(net, vertex.size=as*20, vertex.label = NA, edge.arrow.size=.1, main=\u0026#34;Authorities\u0026#34;)   Let\u0026rsquo;s look for \u0026lsquo;modules\u0026rsquo; within this network. Broadly speaking, these are clumps of nodes that have more or less the same pattern of ties between them, within the group, than without.\n1 2 3  cfg \u0026lt;- cluster_fast_greedy(as.undirected(net)) lapply(cfg, function(x) write.table( data.frame(x), \u0026#39;cfg.csv\u0026#39; , append= T, sep=\u0026#39;,\u0026#39; ))   We create a new variable called cfg and get the cluster_fast_greedy algorithm to perform its calculations. The next line writes the groups to a file, separating each group with an x. (If you tried write.csv as before, you\u0026rsquo;ll get an error message because the output of the algorithm gives a different kind of data type. R is fussy like this.) Examine that file — what groups do you spot? What might these mean, if you went back to the content of the original letters?\nThe line below will plot out our network, colouring it by the communities discerned above, like the following:\n1  plot(cfg, net, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\u0026#34;Communities\u0026#34;)   You can export the plot by clicking on the \u0026lsquo;Export\u0026rsquo; button in the plot panel, to PDF or to PNG. But this is a pretty ugly network. We need to apply a layout to try to make it more visually understandable. There are many different layout options in igraph. We\u0026rsquo;ll assign the layout we want to a variable, and then we\u0026rsquo;ll give that variable to the plot command like the following:\n1 2 3  l1 \u0026lt;- layout_with_fr(net) plot(cfg, net, layout=l1, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\u0026#34;Communities\u0026#34;)   The layout_with_fr is calling the \u0026lsquo;Fruchterman-Reingold\u0026rsquo; algorithm, a kind of layout that imagines each node as a repulsive power, pushing against all the other nodes (it\u0026rsquo;s part of a family of layouts called \u0026lsquo;Forced Atlas\u0026rsquo;). That also means that if you ran the plot command a couple of times, the nodes might end up in different places each time — they are jostling for relative position, and this positioning itself carries no meaning in and of itself (so if a node is at the top of the screen, or the centre of the screen, don\u0026rsquo;t read that to mean anything about importance).\nGood luck! Remember to save your script, and upload the entire project folder to your GitHub repository.\n","description":"Basic Tools","id":24,"section":"week","tags":null,"title":"This week's bonus","uri":"https://craftingdh.netlify.com/week/3/bonus/"},{"content":"Publish data with datasette to heroku\n","description":"Of Macroscopes and Microscopes","id":25,"section":"week","tags":null,"title":"This Week's Bonus","uri":"https://craftingdh.netlify.com/week/4/bonus/"},{"content":" static website with gh-pages static website with jekyll-now static website with expose link back to academic-kickstarter thing  ","description":"Telling the Compelling Story","id":26,"section":"week","tags":null,"title":"Static Websites","uri":"https://craftingdh.netlify.com/week/5/static-websites/"},{"content":" HIST3814O|DIGH3814O Summer 2020 Department of History, Carleton University This is a methods course about learning to use the huge variety of digitized historical resources available in the world, including some perhaps unconventional sources such as social media. You will learn some of the habits of doing born-digital work, including the doing of digital history as an outward facing public history.\nYou do not need to be \u0026lsquo;techy\u0026rsquo; to be successful in this course! You just need to be diligent, honest, and open. You will read, watch, listen to, and discuss the class materials via various online tools including Hypothes.is and Github (Speaking of Hypothes.is, highlight a word on this page and see what happens). We do not use cuLearn in the course. We work on the open web instead. Successful completion of this class involves doing a series of exercises each week designed to push you out of your comfort zone, AND to be a collegial and generous scholar engaging with, and helping your peers to achieve success. What is challenging for one student will not necessarily be challenging for another, and I expect you to push yourself and pull others along as you go. Thus open and honest reporting of what works and what hasn’t worked, is a meaningful aspect of this course. You don’t need to be techy to succeed, but you do need to be willing to embrace when things go ‘wrong’.\nIt\u0026rsquo;s super important that you try to complete the weekly exercises each week before moving onwards. Each week builds on the previous week\u0026rsquo;s work. In my experience, trying to complete this course in a compressed time frame is extremely difficult. Stay on task, complete one module a week. Work that is completed by the end of the week can expect formative feedback from me by the middle of the subsequent week. I prioritize responding to recent work, rather than overdue work. There is no grade penalty for overdue work, but you will miss out on feedback.\nThis course aims to change how you think about, and think with, digitized resources and digital tools. I am not trying to turn you into a coder. Rather, I am trying to turn you into a historian who is thoughtful and reflective about the ways digital tools transform what it is we know about the past and how we come to know it.\n","description":"what to tell your friends","id":27,"section":"docs","tags":null,"title":"1. Course Description","uri":"https://craftingdh.netlify.com/docs/1-coursedescription/"},{"content":"Outcomes  This course will enable you to analyze and assess historical documents, artifacts, and other primary sources through the lens of digital history methods, critically applied, and you will develop awareness of the ways digital tools change what it is possible to know about the past. At the same time, this ability will enable you to analyze and assess critically digital tools and methods from a historical perspective. Given that this is an online course, another outcome will be your ability to conduct such research independently. However, no one operates in a vacuum; digital historians collaborate to troubleshoot or develop technologies, and through open practices to data sharing and reuse, learn to build upon each others\u0026rsquo; work in a collaborative fashion. Another outcome will be the ability to translate the results of digital methods into historical argument. Digital work is often necessarily therefore a kind of public work, and a final outcome therefore will be a professionalized presence online.  Texts See the weekly work section. All readings are open access.\nReal Names Policy You do not need to use your real name or identity on any public-facing work that you do in this course. Nor do you need to explain to me that you wish to use a pseudonym. It is sufficient that you send an email to me with the following message:\n‘I would like to use the following username in all public-facing work: xxxxxxxx’\n…where xxxxx is the name you have selected. For safety’s sake, if you decide to use a pseudonym, do not use one that you have used on any other website or social media platform.\nWhen Life Intervenes There\u0026rsquo;s nothing we can\u0026rsquo;t roll with, in this class. That said, it is a compressed time frame: so if something comes up, just let me know:\nYou don\u0026rsquo;t have to share the details with me. It is enough for me to know that something has intervened. I trust you.\r When something comes up and this course has to move to the backburner, contact me and we can figure out something else to do, or something else that will help you be successful here. It\u0026rsquo;s our course - we can change things up as we need to.\n","description":"what you'll get out of this course","id":28,"section":"docs","tags":null,"title":"2. Learning Outcomes","uri":"https://craftingdh.netlify.com/docs/2-learning-outcomes/"},{"content":"Each week involves completing to the best of your ability a series of exercises; you will document your progress in a weekly log that you will keep online. Log entries are required to be completed by the Sunday evening at the end of the relevant work. A final reflection piece is due at the end of the course. See the assessment page for further details\nWeek 1, May 4 An overview of digital history\nWeek 2, May 11 Basic Tools\nWeek 3, May 19 Basic Tools Encore\nWeek 4, May 25 Of Macroscopes and Microscopes\nWeek 5, June 1 Telling the Compelling Story\nWeek 6, June 8 Bringing it all Together\nJune 16th: Exit Ticket Due The last thing you\u0026rsquo;ll do for this course\n","description":"what happens when, and where","id":29,"section":"docs","tags":null,"title":"3. Course Calendar","uri":"https://craftingdh.netlify.com/docs/3-schedule/"},{"content":"There is no midterm. There is no final exam.\r Weekly Exercises Each week, there are set exercises for you to attempt.\nWeekly work should be completed by the end of the relevant week.\nI am not looking for \u0026lsquo;correct\u0026rsquo; completion, or that you powered through x amount of them. I am, rather, looking for your evidence of thinking through the meaning of the process: what worked, what didn\u0026rsquo;t, why, and what that might mean for you as a historian. I am looking for you to tie your process explicitly to the readings. I am looking to see if the conversations you have with me or your peers (or indeed elsewhere on the web or in other courses) are causing you to reflect on the what/why/how/ of what you do.\nEvidence Therefore, I am looking for the following kinds of evidence (each week will specify what needs to be done):\n logs that keep track of what you actually tried reflection on that process engagement with the materials and your classmates (which might be demonstrated many different ways) evidence for your growth as a historian over this course  At the end of each week, you will provide to me through a form that can be found on each weekly page the links to your evidence for me to consider, by Sunday evening.\nI will return feedback to you within two or three days. I will write you a note giving you my perspective on what you\u0026rsquo;ve done (using the lens of the learning outcomes), and offering advice. Your weekly work will be assessed as \u0026lsquo;satisfactory\u0026rsquo; or \u0026lsquo;unsatisfactory\u0026rsquo;.\nThis feedback will also be mapped against the learning outcomes, so that you know how you are progressing throughout the course.\nThe Exit Ticket You will produce an \u0026lsquo;exit ticket\u0026rsquo; for me at the end of the course (open format) reflecting on where you started and where you\u0026rsquo;ve gotten to, and you will indicate how you feel you\u0026rsquo;ve done against the learning outcomes. The exit ticket is a summary assessment exercise that will pull all the different strings together into a strong cord. Everyone\u0026rsquo;s journey is different. Digital methods are more a matter of practice and time than they are of aptitude.\nYour final grade isn\u0026rsquo;t based on getting something \u0026lsquo;correct\u0026rsquo;. It\u0026rsquo;s based on your own assessment of your own journey.\r If you\u0026rsquo;ve never done digital work before, it might be that you never quite manage to get as many of the tech things working as you might\u0026rsquo;ve wanted: but you now know what you didn\u0026rsquo;t know before. That\u0026rsquo;s a win. You might be a computer science minor and the tech materials don\u0026rsquo;t present you with much challenge: but figuring out how to tell the compelling story was very difficult for you but you\u0026rsquo;re better at it now. Your \u0026lsquo;exit ticket\u0026rsquo; will explain to me your particular context, and it will point to the evidence that demonstrates how you\u0026rsquo;ve moved along from where you were at the beginning to where you are now.\nIf I agree with your assessment, then that is the grade you will receive. When I have disagreed in previous courses this has been, 9.5 times out of 10, to raise the grade: y\u0026rsquo;all are too hard on yourselves. If I have disagreed and felt that you\u0026rsquo;ve overstated things - if you were the 0.5 out of 10 - we would talk and come to an agreement.\nThe Exit Ticket should be submitted by the last day of the early summer term. If you should require more time, you need merely to let me know; no questions asked.\nGrading Remembering the learning outcomes,\n   Learning Outcome A B C D eg. Joe Q Student     1. analytical ability - - - - X   2. methodology - - - - -   3. collaboration - - - - X   4. argumentation - - - - X   5. professionalization - - - - -    \u0026hellip;5/5 would be an A, 4/5 would be a B, 3/5 would be a C, 2/5 a D.\nIn this example, based on my consideration of Joe Q Student\u0026rsquo;s evidence and the feedback I\u0026rsquo;d written him over the six weeks, it seemed to me that he did what he needed to do (satisfactorily) for 3 out of the 5 outcomes and so earned a C. However, his exit ticket might bring him up. Percentage Breakdown I am required by the University to provide a percentage breakdown.\nWeekly work = 80%\nFinal Exit ticket = 20%\nI reserve the right to adjust those percentages to take into account the particular circumstances of the student.\n","description":"grading and ungrading","id":30,"section":"docs","tags":null,"title":"4. Assessment","uri":"https://craftingdh.netlify.com/docs/4-assessment/"},{"content":"The following are the University regulations common to all History courses.\r COPIES OF WRITTEN WORK SUBMITTED Always retain for yourself a copy of all essays, term papers, written assignments or take-home tests submitted in your courses.\nPLAGIARISM The University Senate defines plagiarism as “presenting, whether intentionally or not, the ideas, expression of ideas or work of others as one’s own.” This can include:\n reproducing or paraphrasing portions of someone else’s published or unpublished material, regardless of the source, and presenting these as one’s own without proper citation or reference to the original source; submitting a take home examination, essay, laboratory report or other assignment written, in whole or in part, by someone else; using ideas or direct, verbatim quotations, or paraphrased material, concepts, or ideas without appropriate acknowledgment in any academic assignment; using another’s data or research findings; failing to acknowledge sources through the use of proper citations when using another’s works and/or failing to use quotation marks; handing in \u0026ldquo;substantially the same piece of work for academic credit more than once without prior written permission of the course instructor in which the submission occurs.\u0026rdquo;  Plagiarism is a serious offence which cannot be resolved directly with the course’s instructor. The Associate Dean of the Faculty conducts a rigorous investigation, including an interview with the student, when an instructor suspects a piece of work has been plagiarized. Penalties are not trivial. They can include a final grade of \u0026ldquo;F\u0026rdquo; for the course.\nCOURSE SHARING WEBSITES and COPYRIGHT To the degree that I am able, all original content on this course website is released under creative commons licensed. That means you may copy and share and reuse it, but you must attribute under the following terms - click through..\nSTATEMENT ON CLASS CONDUCT The Carleton University Human Rights Policies and Procedures affirm that all members of the University community share a responsibility to:\n promote equity and fairness, respect and value diversity, prevent discrimination and harassment, and preserve the freedom of its members to carry out responsibly their scholarly work without threat of interference.  Carleton University Equity Services states that \u0026lsquo;every member of the University community has a right to study, work and live in a safe environment free of discrimination or harassment\u0026rsquo;. [In May of 2001 Carleton University’s Senate and Board of Governors approved the Carleton University Human Rights Policies and Procedures. The establishment of these policies and procedures was the culmination of the efforts of the Presidential Advisory Committee on Human Rights and a Human Rights Implementation Committee.]\nGRADING SYSTEM Letter grades assigned in this course will have the following percentage equivalents:\n   Grade        A+ = 90-100 (12) B = 73-76 (8) C - = 60-62 (4) F= 0-49 (0) – Failure: no academic credit   A = 85-89 (11) B - = 70-72 (7)\tD+ = 57-59 (3)    A - = 80-84 (10) C+ = 67-69 (6) D = 53-56 (2)    B+ = 77-79 (9) C = 63-66 (5) D - = 50-52 (1)     The following additional final course grades may be assigned by instructors:\nDEF Official deferral of final exam (see \u0026ldquo;Petitions to Defer\u0026rdquo;)\nGNA Grade not available. This is used when there is an allegation of an academic offence. The notation is replaced with the appropriate grade for the course as soon as it is available.\nIP\tIn Progress – a notation (IP) assigned to a course by a faculty member when: At the undergraduate level, an undergraduate thesis or course has not been completed by the end of the period of registration.\nWDN\tWithdrawn. No academic credit, no impact on the CGPA. WDN is a permanent notation that appears on the official transcript for students who withdraw after the full fee adjustment date in each term (noted in the Academic Year section of the Calendar each term). Students may withdraw on or before the last day of classes.\nStanding in a course is determined by the course instructor subject to the approval of the Faculty Dean. This means that grades submitted by the instructor may be subject to revision. No grades are final until they have been approved by the Dean.\nWITHDRAWAL WITHOUT ACADEMIC PENALTY May 22, 2020: Last day for a full fee adjustment when withdrawing from early summer and full summer courses (financial withdrawal). Withdrawals after this date will result in a permanent notation of WDN on the official transcript.\nJune 16, 2020: Last day for academic withdrawal from early summer courses.\nJuly 17, 2020: Last day for a full fee adjustment when withdrawing from late summer courses (financial withdrawal).\nAugust 14, 2020: Last day for academic withdrawal from late summer and full summer courses and any other courses that end this term.\nREQUESTS FOR ACADEMIC ACCOMMODATIONS You may need special arrangements to meet your academic obligations during the term. For an accommodation request the processes are as follows:\nPregnancy obligation: write to the professor with any requests for academic accommodation during the first two weeks of class, or as soon as possible after the need for accommodation is known to exist. For more details see https://carleton.ca/equity/wp-content/uploads/Student-Guide-to-Academic-Accommodation.pdf\nReligious obligation: write to the professor with any requests for academic accommodation during the first two weeks of class, or as soon as possible after the need for accommodation is known to exist. For more details see https://carleton.ca/equity/wp-content/uploads/Student-Guide-to-Academic-Accommodation.pdf\nAccommodation for Student Activities: write to the professor with any requests for academic accommodation during the first two weeks of class, or as soon as possible after the need for accommodation is known to exist. For more details see https://carleton.ca/senate/wp-content/uploads/Accommodation-for-Student-Activities-1.pdf\nSurvivors of sexual violence: As a community, Carleton University is committed to maintaining a positive learning, working and living environment where sexual violence will not be tolerated, and is survivors are supported through academic accommodations as per Carleton\u0026rsquo;s Sexual Violence Policy. For more information about the services available at the university and to obtain information about sexual violence and/or support, visit: https://carleton.ca/sexual-violence-support/wp-content/uploads/Sexual-Violence-Policy-December-1-2016.pdf\nAcademic Accommodations for Students with Disabilities: The Paul Menton Centre for Students with Disabilities (PMC) provides services to students with Learning Disabilities (LD), psychiatric/mental health disabilities, Attention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorders (ASD), chronic medical conditions, and impairments in mobility, hearing, and vision. If you have a disability requiring academic accommodations in this course, please contact PMC at 613-520-6608 or pmc@carleton.ca for a formal evaluation. If you are already registered with the PMC, contact your PMC coordinator to send me your Letter of Accommodation at the beginning of the term, and no later than two weeks before the first in-class scheduled test or exam requiring accommodation (if applicable). After requesting accommodation from PMC, meet with me to ensure accommodation arrangements are made. Please consult the PMC website for the deadline to request accommodations for the formally-scheduled exam (if applicable).\nPETITIONS TO DEFER Students unable to write a final examination because of illness or other circumstances beyond their control or whose performance on an examination has been impaired by such circumstances may apply within five working days to the Registrar\u0026rsquo;s Office for permission to write a deferred examination. The request must be fully and specifically supported by a medical certificate or other relevant documentation. Only deferral petitions submitted to the Registrar\u0026rsquo;s Office will be considered.\nADDRESSES (613-520-2600, phone ext.)  Department of History (2828) 400 PA Registrar’s Office (3500) 300 Tory Academic Advising Centre (7850) 302 Tory Paul Menton Centre (6608) 500 Unicentre Centre for Student Academic Support – Study Skills, Writing Tutorials, Bounce Back (3822) 4th fl Library  Application for Graduation Deadlines\n Spring Graduation (June): April 1 Fall Graduation (November): September 1 Winter Graduation (February): December 1  ","description":"university rules","id":31,"section":"docs","tags":null,"title":"5. Common Regulations","uri":"https://craftingdh.netlify.com/docs/5-commonregs/"},{"content":"Dr. Graham can be found online at\nshawn dot graham at carleton dot ca\nor on Twitter at @electricarchae.\nHe will be present in our Crafting Digital History discord server every day.\nAlso, keep an eye on the \u0026lsquo;news\u0026rsquo; tab for quick updates, when necessary.\nIf we ever get out of quarantine, and you\u0026rsquo;re in Ottawa, he often haunts the Library coffeeshop.\n","description":"how to find him","id":32,"section":"docs","tags":null,"title":"6. Contacting Dr. Graham","uri":"https://craftingdh.netlify.com/docs/6-contact/"},{"content":"This course originally was a face-to-face class, over 13 weeks. That let us get into the nitty-gritty of different approaches and platforms; it gave us the opportunity to have guest speakers; and most importantly, it gave us the time to really grapple with different kinds of historical \u0026lsquo;capta\u0026rsquo; (see Drucker on \u0026lsquo;capta versus data\u0026rsquo;). But, needs evolved and eventually the course moved fully online. Now only 6 weeks long, a lot of things have been cut, compressed, or rethought entirely. I do not aim for coverage. Rather, I am trying to help you learn the skills that you will need to uncover whatever aspect of digital history method and thought that will help you with your research goals. A big part of that is trying to teach how to deal with what might feel like \u0026lsquo;failure\u0026rsquo;, on first blush.\nSocial Contact Another problem with moving fully online was the isolation. Working with digital tools, especially when you\u0026rsquo;re not overly familiar with how they work - or even the underlying metaphors that would help you make sense of what\u0026rsquo;s going on - is frustrating. Bad enough when you have classmates you see twice a week and can at least complain with over coffee later; extremely awful when you\u0026rsquo;re on a bad internet connection and everyone thinks that you\u0026rsquo;re not really doing schoolwork.\nInitially, I used Slack as a way of trying to meet that need for human connection that makes learning meaningful. It worked, more or less, but if there was a conversation going on and you missed part of it, it could just feel overwhelming and impenetrable.\nAnd it was one more damned thing to install. One more damned thing to get a password for.\nI tried Zulipchat another year. Same problem again.\nThis year, as I write this, I\u0026rsquo;m trying Discord, for its voice and screensharing integration. We\u0026rsquo;ll see how that goes.\nThe Workbook I put all of the materials for the course into an online workbook, built from text files that used markdown conventions to indicate headings, links, images and so on. The website for the workbook would then be generated using mkdocs, a python library for generating static websites. It worked, but over the years I kept adding more and more to it, links would get broken or websites would go offline\u0026hellip; it was a lot of work to prune it, keep it up to date.\nAnd then COVID-19 happened.\nI normally have a lot of time to rethink what needs taught, to prune what has become useless, to add newer resources, newer thinking. But this time around, not so much. I put the older version of the course away, and started trying to build what I thought were the most useful elements: given everything else that is going on, what are the key things I think you ought to do in order to plant the seeds of your eventual engagement with digital history?\nThe result is this present website, which combines a number of features of the course that were previously spread across a number of locations. I have also pared down to a more limited number of skill-based exercises so that you have more time to think about the context (historiographical, theoretical) of their potential use. I offer more direction and less freedom-to-choose in terms of the work I think that needs to be done, but the trade off here is that you are more able to share what you are doing with your peers and to find help.\nWhich reminds me:\nDigital History is a team sport. You are never expected to power through all of this material on your own in heroic scholarly endeavor. If you need help, ask for help; if you can help, offer it. You are not alone.\r This website is built using Hugo, a static site builder. Static sites are quicker, more secure, and separate content from container, thus making them more sustainable. I write all of the content in individual text files, which I can then turn into whatever output - html, pdf, word doc - that I need. My writing is freed from subscription-based software that might lock it in. I push all of the text files onto github (you can see them here.) Then, I have netlify.com watch those files for any changes. When it spots changes, it uses Hugo to turn them into html, and serves them up at craftingdh.netlify.com.\nSome of the work we do requires working with the command line or the terminal of your computer; I also provide a virtual computer that you can access through your browser so that you don\u0026rsquo;t have to worry about configuring your particular machine to do any one task. This also allows me to write instructions once, and know that everyone will be able to do it (there are always differences between Mac and PC that trip us up). The virtual computer is provided by a service called Binder; I set up a folder on Github with the required bits-and-pieces (think of them as lego blocks) that are necessary for a particular task. I then point Binder at that folder, and it spins up a virtual computer with those pieces properly installed. It then gives me a URL that I can share with you - link on it, and boom! a computer in the cloud ready to go.\nAll of this is now under the \u0026lsquo;weekly work\u0026rsquo; part of this site. Take your time, annotate the parts that are confusing or troublesome with hypothes.is and you\u0026rsquo;ll do well.\nOne last thing I asked Twitter for its thoughts, as you do:\n.twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  5 top [method/skills] things students new to digital history should learn. Go!\n\u0026mdash; Shawn Graham (@electricarchaeo) March 28, 2020 \u0026hellip;follow the thread!\n","description":"how the course tech is built; how the course came to be","id":33,"section":"docs","tags":null,"title":"7. Colophon","uri":"https://craftingdh.netlify.com/docs/7-colophon/"},{"content":"Help! Things have gone wrong! Montparnasse Derailment, 1895\nThings are going to go wrong. Things are going to break. Instructions will seem vague or incomplete - or worse, I will have indeed forgotten to tell you something crucial.\nOr worse, I\u0026rsquo;ve made an assumption about your understanding, and left things unsaid that needed to be made explicit. There\u0026rsquo;s a lot of this, in the digital world, in fact.\nThis week, you learned a bit about Git and Github. Let\u0026rsquo;s look at a question on the q-and-a site, \u0026lsquo;Stackoverflow\u0026rsquo; here. Just scroll through that question, and look at some of the answers. Make a note of any terms or code or responses that make not an ounce of sense to you.\nIt can make you feel rather overwhelmed, rather quickly. And like most places on the internet, StackOverflow has its own culture, its own norms of expression and behaviour. People who are new can get roasted. That\u0026rsquo;s one of the reasons we have our own discord server, and a private reading group on Hypothesis: these are places for you to ask for help from a community that understands where you\u0026rsquo;re coming from because we\u0026rsquo;re all in the same headspace.\nSo, when things go wrong, the first thing to do is to find the community that understands where you\u0026rsquo;re coming from. I read and search StackOverflow all the time, but I rarely ask questions there. Instead, I write to trusted colleagues. And they in turn write to me.\nBut there are a couple of things we can do to make life easier.\nAsking good questions StackOverflow does have good advice on asking good questions. I will excerpt some of their advice:\n Write a title that summarizes the specific problem\u0026hellip; If you\u0026rsquo;re having trouble summarizing the problem, write the title last - sometimes writing the rest of the question first can make it easier to describe the problem.\n  Introduce the problem before you post any code [or a screenshot or a video clip]\u0026hellip;start by expanding on the summary you put in the title. Explain how you encountered the problem you\u0026rsquo;re trying to solve, and any difficulties that have prevented you from solving it yourself.\n  Help others reproduce the problem [SG: post the code, post a screenshot, post a video clip walk through]\n Please do this.\nEvery term, I receive dozens of emails with the subject line \u0026lsquo;help\u0026rsquo; and the message body \u0026lsquo;i tried it and it doesn\u0026rsquo;t work\u0026rsquo;. Sometimes, the message will also say \u0026lsquo;i worked on it for 3 hrs and it doesn\u0026rsquo;t work\u0026rsquo;.\nNone of that is helpful. Telling me that you worked on it for 3 hours just shows me that you ignored my messages about \u0026lsquo;if it doesn\u0026rsquo;t come after 30 minutes, take a break, ask for help\u0026rsquo;. So - let\u0026rsquo;s practice right now.\nWe\u0026rsquo;re going to make a git mistake.\nHere\u0026rsquo;s the situation:\n Imagine that you were working on your repo, wanting to update it from the command line - but you had to go away for a bit. When you come back, you discover your computer has run out of juice and shut down. Plugging it back in, none of your folders are open. You click on what you think is the right one and start to carry on\u0026hellip;.\n Right-click on any folder on your computer except the one(s) you created in the github exercise, and open a command prompt or a terminal there. Imagine I told you to commit your changes, like so: git commit -m \u0026quot;this is just a test\u0026quot;.\nDo that.\nWhat happened? Write a \u0026lsquo;help request\u0026rsquo;, using the guidelines above, and post it in the appropriate channel in our Discord, introducing it with an appropriate title (eg, \u0026lsquo;Demo Help Request\u0026rsquo;). Feel free to respond to a request with eg \u0026lsquo;Demo Help Response\u0026rsquo;.\nAnswering questions As the course progresses, you will have many opportunities to ask for help from me and from your peers - post these in our discord server in the appropriate channel.\nSometimes, you will see questions that you can answer. A few guidelines for answering questions:\n There are many different ways to solve these issues; someone might post a different way but that doesn\u0026rsquo;t mean that either of you are necessarily wrong. Read the question carefully. Request clarification if there are bits that the original poster didn\u0026rsquo;t make clear - help them to ask better questions. Use screenshots and screenvideo (I find screencastomatic handy in this regard) to show how things worked for you. Provide links to useful relevant material if you know of it. Assume the best of everyone: not everyone has the same fluency with these machines, and it takes a while to learn the language.  ","description":"instructions","id":34,"section":"week","tags":null,"title":"Asking for help","uri":"https://craftingdh.netlify.com/week/1/help/"},{"content":"Historian\u0026rsquo;s don\u0026rsquo;t deal with video very well. That is to say, we don\u0026rsquo;t have many tools for dealing with video as video, as a moving image, and analyzing with an eye towards the specific affordances of the medium (but see Arnold and Tilton\u0026rsquo;s \u0026lsquo;Distant Viewing\u0026rsquo; project, and their position paper.)\nBut we\u0026rsquo;re good with text. In this bonus exercise, you\u0026rsquo;ll download a video from youtube of Dr. Martin Luther King\u0026rsquo;s \u0026lsquo;I have a dream\u0026rsquo; speech (wikipedia), and use Google\u0026rsquo;s speech-to-text engine to generate a time-stamped transcript of the speech.\nNB. This works with historic audio recordings too, but of course, the better the audio quality, the better the transcript. You could even record yourself speaking and use this to make a transcript of audio notes.\nGet the code We\u0026rsquo;re going to use this repository by Alex Kras; you can read his original blog post describing his project here.\n  Open terminal or anaconda powershell, and make a new python environment with conda create -n audio python=3.7 anaconda\n  activate your environment: conda activate audio\n  get the repository with: git clone https://github.com/akras14/speech-to-text.git\n  change directories so that you\u0026rsquo;re in that repo: cd speech-to-text\n  install the necessary bits-and-pieces that Kras put into his requirements file: pip install -r requirements.txt\n  Getting access to the Google API Follow Kras\u0026rsquo;s instructions to get an api key to access the speech-to-text engine:\n Sign up for the free tier and then sign in to Google Cloud Console Click “APIs \u0026amp; Services” Click “Credentials” Click “Create Credentials” Select “Service Account Key” Under “Service Account” select “New service account” Name service (whatever you’d like) Select Role: “Project” -\u0026gt; “Owner” Leave “JSON” option selected Click “Create” Save generated API key file Rename file to api-key.json  The file api-key.json needs to be saved inside your copy of the speech-to-text folder (eg, \\speech-to-text\\api-key.json). Note that Google sometimes changes up its interface a bit, and so the exact sequence of clicks might not be the same as described above.\nSome video utilities   Download and install youtube-dl or install it from the command line:\npip install --upgrade youtube_dl.\nIf you try the command line and get an error about not having permission to do this, try\nsudo pip install --upgrade youtube_dl\nThis will require you to enter your computer username and password. Note that when you enter the password, the cursor won\u0026rsquo;t move. This is old-school security in action\u0026hellip;\n  Download and install ffmpeg.\nOn a mac - if you haven\u0026rsquo;t already, get homebrew first (a package manager for downloading and installing things like this) here. That website gives you a command to run in your terminal to download and install homebrew. (But if you successfully installed Brew when you did the wget work, then you just need to do the brew install command). Then, when homebrew is installed: brew install ffmpeg\n  On a pc: download and install from here\nNow let\u0026rsquo;s do this!  There\u0026rsquo;s a copy of the \u0026lsquo;I have a dream\u0026rsquo; speech uploaded by this user at: https://www.youtube.com/watch?v=I47Y6VHc3Ms. Note the bit after ?v=. That\u0026rsquo;s the id of the video we\u0026rsquo;re after. There are several versions of each video on youtube, to account for the different kinds of devices and resolution and so on. We\u0026rsquo;ll use youtube-dl to examine the different versions of this video, so that we can find the smallest one. At the command prompt, type youtube-dl -F I47Y6VHc3Ms The -F (upper case!) tells youtube-dl to go grab all of the info and print it out for us. Each line is a different version of the video, and each line has a unique number id. We look for the line that tells us which version is smallest. Today, that\u0026rsquo;s line 249 (notice that it also tells you that this is just the audio only). So now we tell youtube-dl to download only that version: youtube-dl -f 249 I47Y6VHc3Ms. The -f (lower case!) flag lets us select the line number we want, and then the ID string tells youtube which video we\u0026rsquo;re after. Use your file explorer to rename the file that you just downloaded to something sensible; I changed mine to \u0026lsquo;mlk.webm\u0026rsquo;. Now we\u0026rsquo;ll convert the file to a .wav file with the ffmpeg command: ffmpeg -i mlk.webm mlk.wav Move the file into the /source folder. Now we\u0026rsquo;re ready to chop the file into 30 second increments. Here we go: ffmpeg -i source/mlk.wav -f segment -segment_time 30 -c copy parts/out%09d.wav But before we pass those to google, note that files that contain dead air or nothing that is recognizable as speech can break things. Listen to the first two or three files you just created (in your file explorer, click on them). Delete the ones that are dead air. \u0026hellip;and now pass on the remaining filesto Google for processing! Make sure you\u0026rsquo;re in the correct location (eg, the speech-to-text folder) and type python slow.py \u0026gt; results.txt  A little progress bar will appear, and your code is passing your files one at a time up to google for processing; google is sending back to you the text! When it\u0026rsquo;s done, just open up the results.txt file.\nIf you have audio from other sources you\u0026rsquo;d like to try, just follow the pattern for converting it to .wav format, place it in the correct place (\\source), clean out the \\parts subdirectory, modify the command in step 7 with your own file name, and away you\u0026rsquo;ll go!\n","description":"instructions","id":35,"section":"week","tags":null,"title":"This week's bonus","uri":"https://craftingdh.netlify.com/week/2/bonus/"},{"content":"The Last Bit The last bit of the course is the Exit Ticket.\nDo The format of the exit ticket is up to you. You craft a statement reflecting on where you started and where you’ve gotten to, and you will indicate how you feel you’ve done against the learning outcomes. You may, if you wish, indicate what you think your letter grade ought to be, and why.\nThink about your journey, and where you started. How have you grown? What successes did you have? Did you have any glorious failures? Did you help others - or did someone else make a difference for you? How did week 6 pan out for you? Could you take what you\u0026rsquo;ve learned and apply it in another course - or do you still fee hesitant about certain aspects? Can you contrast what you now know about digital history with what you thought about the subject when you started out back in May?\nThis does not need to be much more than a single page (or the equivalent in other formats; how you measure that is up to you).\nSubmit work You can submit the link to your work on this form\nCelebrate This hasn\u0026rsquo;t been - I\u0026rsquo;ll wager - an easy journey for you. But it will pay you back. I promise. Thank you for joining me!\n","description":"Final Submission Instructions","id":36,"section":"week","tags":null,"title":"Final Submission Instructions","uri":"https://craftingdh.netlify.com/week/6-5/instructions/"},{"content":" exhibit with Wax Omeka - reclaimhosting, links to tutorials Neatline - links to tutorials  ","description":"Telling the Compelling Story","id":37,"section":"week","tags":null,"title":"This Week's Bonus","uri":"https://craftingdh.netlify.com/week/5/bonus/"},{"content":"Click title to see the full Post\nLorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\ninhospita parcite confusaque translucet patri vestro qui optatis\nlumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus\nsilentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria\ntractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra\ndicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere\nfurit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli\nLelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare\nEchionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert\nausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae\nvulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem\nPropoetides parte.\n","description":"","id":38,"section":"blog","tags":null,"title":"Placeholder Text","uri":"https://craftingdh.netlify.com/blog/placeholder/"}]